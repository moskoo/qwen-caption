#!/bin/bash

echo "ğŸš€ é€šä¹‰åƒé—®ç¦»çº¿å›¾ç‰‡æ‰“æ ‡å·¥å…·å¯åŠ¨è„šæœ¬ (Python 3.10 + PyTorch 2.9.1)"
echo "âœ… ä¸“ä¸ºPython 3.10ç¯å¢ƒä¼˜åŒ–"
echo "âœ… ä½¿ç”¨Torch 2.2.2æœ€æ–°ç¨³å®šç‰ˆ"
echo "âœ… ä¿®å¤transformers_stream_generatorä¾èµ–è¯†åˆ«é—®é¢˜"
echo "âœ… é€‚é…Qwen-VL-Chatæ–‡ä»¶ç»“æ„"
echo "=============================================="

# è®¾ç½®ç¯å¢ƒå˜é‡
export HF_HUB_ENABLE_HF_TRANSFER=1
export HF_HUB_DOWNLOAD_TIMEOUT=300
export TRANSFORMERS_NO_ADVISORY_WARNINGS=1
export TOKENIZERS_PARALLELISM=false
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# æ£€æŸ¥Python 3.10æ˜¯å¦å®‰è£…
if ! command -v python3.10 &> /dev/null; then
    echo "âŒ Python 3.10 æœªå®‰è£…"
    echo "ğŸ’¡ è¯·å…ˆå®‰è£…Python 3.10"
    echo "   Ubuntu/Debian: sudo apt update && sudo apt install python3.10 python3.10-venv"
    echo "   CentOS/RHEL: sudo yum install python3.10 python3.10-venv"
    echo "   macOS: brew install python@3.10"
    echo "ã€1/4ã€‘æ£€æŸ¥å¹¶å®‰è£…python3.10-venvä¾èµ–..."
    if ! dpkg -s python3.10-venv &> /dev/null; then
        echo "æœªå®‰è£…python3.10-venvï¼Œå¼€å§‹å®‰è£…..."
        sudo apt update && sudo apt install -y python3.10-venv python3.10-dev
    else
        echo "python3.10-venvå·²å®‰è£…"
    fi
    exit 1
fi

# æ£€æŸ¥GPUå’ŒCUDA (å¢å¼ºç‰ˆ)
check_gpu_requirements() {
    local has_gpu=false
    local cuda_available=false
    local cuda_version=""
    local gpu_details=""
    
    echo "ğŸ” æ£€æµ‹GPUå’ŒCUDAçŠ¶æ€..."
    
    # 1. é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰NVIDIA GPU
    if command -v nvidia-smi &> /dev/null; then
        # è·å–GPUè¯¦ç»†ä¿¡æ¯
        gpu_details=$(nvidia-smi --query-gpu=name,memory.total --format=csv,noheader 2>/dev/null || echo "")
        
        if [ -n "$gpu_details" ]; then
            has_gpu=true
            echo "ğŸ® æ£€æµ‹åˆ°NVIDIA GPU:"
            echo "   $gpu_details"
        else
            echo "âš ï¸  nvidia-smiå­˜åœ¨ä½†æœªæ£€æµ‹åˆ°GPUï¼Œå¯èƒ½é©±åŠ¨é—®é¢˜"
        fi
    else
        echo "ğŸ” æœªæ‰¾åˆ°nvidia-smiå‘½ä»¤ï¼Œå°è¯•å…¶ä»–æ£€æµ‹æ–¹æ³•..."
        
        # å¤‡ç”¨æ–¹æ³•1: æ£€æŸ¥/proc/driver/nvidia
        if [ -d "/proc/driver/nvidia" ]; then
            echo "ğŸ® é€šè¿‡/proc/driver/nvidiaæ£€æµ‹åˆ°NVIDIA GPU"
            has_gpu=true
        fi
        
        # å¤‡ç”¨æ–¹æ³•2: æ£€æŸ¥lspci
        if command -v lspci &> /dev/null; then
            if lspci 2>/dev/null | grep -i nvidia &> /dev/null; then
                echo "ğŸ® é€šè¿‡lspciæ£€æµ‹åˆ°NVIDIA GPU"
                has_gpu=true
            fi
        fi
        
        # å¤‡ç”¨æ–¹æ³•3: æ£€æŸ¥è®¾å¤‡æ–‡ä»¶
        if [ -c "/dev/nvidia0" ] || [ -c "/dev/nvidiactl" ]; then
            echo "ğŸ® é€šè¿‡è®¾å¤‡æ–‡ä»¶æ£€æµ‹åˆ°NVIDIA GPU"
            has_gpu=true
        fi
    fi
    
    # 2. æ£€æŸ¥CUDAå¯ç”¨æ€§
    if $has_gpu; then
        echo "ğŸ” æ£€æŸ¥CUDAå·¥å…·åŒ…..."
        
        # æ£€æŸ¥CUDAè¿è¡Œæ—¶API
        if python3.10 -c "
import importlib.util, sys, subprocess, json, os, math, random, time, datetime, collections, itertools, fractions, decimal, typing, statistics, heapq, bisect, copy, string, re, collections, math
try:
    import torch
    if torch.cuda.is_available():
        print('CUDA_AVAILABLE: true')
        print(f'GPU_COUNT: {torch.cuda.device_count()}')
        print(f'CURRENT_DEVICE: {torch.cuda.current_device()}')
        print(f'GPU_NAME: {torch.cuda.get_device_name(0)}')
        print(f'GPU_MEMORY: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB')
        print(f'PYTORCH_VERSION: {torch.__version__}')
    else:
        print('CUDA_AVAILABLE: false')
except Exception as e:
    print(f'ERROR: {str(e)}')
" 2>/dev/null | grep -q "CUDA_AVAILABLE: true"; then
            cuda_available=true
            echo "âœ… CUDAåœ¨PyTorchä¸­å¯ç”¨"
            
            # è·å–PyTorchæ£€æµ‹åˆ°çš„CUDAç‰ˆæœ¬
            cuda_version=$(python3.10 -c "import torch; print(torch.version.cuda)" 2>/dev/null || echo "")
            if [ -n "$cuda_version" ]; then
                echo "ğŸ”¢ PyTorchä½¿ç”¨çš„CUDAç‰ˆæœ¬: $cuda_version"
            fi
        else
            echo "âš ï¸  GPUå­˜åœ¨ä½†CUDAåœ¨PyTorchä¸­ä¸å¯ç”¨"
            echo "ğŸ’¡ å¯èƒ½åŸå› :"
            echo "   â€¢ æœªå®‰è£…æ­£ç¡®ç‰ˆæœ¬çš„PyTorch"
            echo "   â€¢ NVIDIAé©±åŠ¨ç‰ˆæœ¬è¿‡ä½"
            echo "   â€¢ CUDAå·¥å…·åŒ…æœªæ­£ç¡®é…ç½®"
            
            # æ£€æŸ¥nvcc
            if command -v nvcc &> /dev/null; then
                cuda_version=$(nvcc --version 2>/dev/null | grep release | sed 's/.*release //' | sed 's/,.*//')
                echo "ğŸ”¢ æ£€æµ‹åˆ°CUDAå·¥å…·åŒ…ç‰ˆæœ¬: $cuda_version"
            else
                # å°è¯•ä»nvidia-smiè·å–
                cuda_version=$(nvidia-smi 2>/dev/null | grep "CUDA Version" | awk '{print $9}' 2>/dev/null || echo "")
                if [ -n "$cuda_version" ]; then
                    echo "ğŸ”¢ ä»nvidia-smiè·å–CUDAç‰ˆæœ¬: $cuda_version"
                else
                    echo "âš ï¸  æ— æ³•ç¡®å®šCUDAç‰ˆæœ¬"
                fi
            fi
            
            # æ£€æŸ¥é©±åŠ¨ç‰ˆæœ¬
            if command -v nvidia-smi &> /dev/null; then
                driver_version=$(nvidia-smi 2>/dev/null | grep "Driver Version" | awk '{print $6}' 2>/dev/null || echo "")
                if [ -n "$driver_version" ]; then
                    echo "ğŸ”§ NVIDIAé©±åŠ¨ç‰ˆæœ¬: $driver_version"
                    
                    # ç²—ç•¥æ£€æŸ¥é©±åŠ¨ä¸CUDAå…¼å®¹æ€§
                    if [ -n "$cuda_version" ]; then
                        if [[ "$cuda_version" =~ 12 ]] && [[ "$driver_version" < "525" ]]; then
                            echo "âš ï¸  è­¦å‘Š: CUDA 12.xéœ€è¦NVIDIAé©±åŠ¨ç‰ˆæœ¬>=525"
                        elif [[ "$cuda_version" =~ 11.[0-8] ]] && [[ "$driver_version" < "450" ]]; then
                            echo "âš ï¸  è­¦å‘Š: CUDA 11.xéœ€è¦NVIDIAé©±åŠ¨ç‰ˆæœ¬>=450"
                        fi
                    fi
                fi
            fi
        fi
    else
        echo "ğŸ’» æœªæ£€æµ‹åˆ°NVIDIA GPUï¼Œå°†ä½¿ç”¨CPUç‰ˆæœ¬"
    fi
    
    # 3. æœ€ç»ˆçŠ¶æ€æŠ¥å‘Š
    echo "ğŸ“Š GPUæ£€æµ‹ç»“æœ:"
    echo "   â€¢ GPUå­˜åœ¨: $has_gpu"
    echo "   â€¢ CUDAå¯ç”¨: $cuda_available"
    echo "   â€¢ CUDAç‰ˆæœ¬: ${cuda_version:-'æœªçŸ¥'}"
    
    # 4. è¿”å›ç»“æœ
    echo "$has_gpu:$cuda_available:$cuda_version"
}

# æ£€æŸ¥ç£ç›˜ç©ºé—´
check_disk_space() {
    local free_space_gb
    if command -v df &> /dev/null; then
        free_space_gb=$(df -BG . 2>/dev/null | awk 'NR==2 {print $4}' | sed 's/G//')
    else
        free_space_gb=25  # æ— æ³•æ£€æŸ¥æ—¶å‡è®¾è¶³å¤Ÿ
    fi

    if [ -z "$free_space_gb" ]; then
        free_space_gb=25
    fi

    echo "ğŸ’¾ å¯ç”¨ç£ç›˜ç©ºé—´: ${free_space_gb}GB"

    if [ "$free_space_gb" -lt 20 ]; then
        echo "âš ï¸  è­¦å‘Š: Qwen-VL-Chatæ¨¡å‹éœ€è¦çº¦18GBç©ºé—´ï¼Œå»ºè®®è‡³å°‘20GBç©ºé—²ç©ºé—´"
        read -p "ç»§ç»­å®‰è£…? (y/n): " confirm
        if [ "$confirm" != "y" ]; then
            exit 1
        fi
    fi
}

# è·å–PyTorchå®‰è£…å‘½ä»¤ (Python 3.10å…¼å®¹)
get_pytorch_install_command() {
    local has_gpu="$1"
    local cuda_available="$2"
    local cuda_version="$3"

    # PyTorch 2.10.0 for Python 3.10
    if [ "$has_gpu" = "true" ] && [ "$cuda_available" = "true" ]; then
        if [[ "$cuda_version" =~ 12 ]]; then
            echo "pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121 --no-cache-dir --quiet"
        elif [[ "$cuda_version" =~ 11\.[4-9] ]]; then
            echo "pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118 --no-cache-dir --quiet"
        else
            echo "torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cpu --no-cache-dir --quiet"
        fi
    else
        echo "pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cpu --no-cache-dir --quiet"
    fi
}

# è®¾ç½®Python 3.10è™šæ‹Ÿç¯å¢ƒ
setup_virtual_env() {
    local env_name=".venv"
    local python_cmd="python3.10"

    echo "ğŸ”„ æ£€æŸ¥Python 3.10è™šæ‹Ÿç¯å¢ƒ: $env_name"

    # æ£€æŸ¥ç¯å¢ƒæ˜¯å¦å­˜åœ¨
    if [ -d "$env_name" ]; then
        echo "âœ… è™šæ‹Ÿç¯å¢ƒ '$env_name' å·²å­˜åœ¨"

        # æ¿€æ´»ç¯å¢ƒ
        source "$env_name/bin/activate"
        echo "âœ… è™šæ‹Ÿç¯å¢ƒ '$env_name' å·²æ¿€æ´»!"

        # éªŒè¯ç¯å¢ƒ
        echo "ğŸ› ï¸  éªŒè¯å½“å‰Python 3.10è·¯å¾„å’Œç¯å¢ƒ..."
        echo "å½“å‰Python: $(which python)"
        current_python_version=$(python -c "import platform; print(platform.python_version())")
        echo "å½“å‰Pythonç‰ˆæœ¬: $current_python_version"

        # æ£€æŸ¥æ˜¯å¦ä¸º3.10
        if [[ "$current_python_version" != 3.10* ]]; then
            echo "âŒ å½“å‰ç¯å¢ƒPythonç‰ˆæœ¬ä¸º $current_python_versionï¼Œä¸æ˜¯3.10"
            echo "ğŸ’¡ é‡æ–°åˆ›å»ºPython 3.10è™šæ‹Ÿç¯å¢ƒ..."
            rm -rf "$env_name"
            $python_cmd -m venv "$env_name"
            source "$env_name/bin/activate"
        fi

        # ä¿®å¤ä¾èµ–
        fix_dependencies
    else
        echo "ğŸ“¦ åˆ›å»ºæ–°çš„Python 3.10è™šæ‹Ÿç¯å¢ƒ: $env_name"

        # åˆ›å»ºç¯å¢ƒ
        $python_cmd -m venv "$env_name"

        # æ¿€æ´»ç¯å¢ƒ
        source "$env_name/bin/activate"

        # éªŒè¯ç¯å¢ƒ
        echo "âœ… è™šæ‹Ÿç¯å¢ƒ '$env_name' å·²åˆ›å»ºå¹¶æ¿€æ´»"
        echo "å½“å‰Python: $(which python)"
        current_python_version=$(python -c "import platform; print(platform.python_version())")
        echo "å½“å‰Pythonç‰ˆæœ¬: $current_python_version"

        # å®‰è£…ä¾èµ–
        install_dependencies
    fi

    # æ ‡è®°ç¯å¢ƒå·²å®‰è£…
    touch .env_installed

    # æœ€ç»ˆéªŒè¯
    echo "ğŸ” æœ€ç»ˆä¾èµ–éªŒè¯..."
    verify_dependencies
}

# ä¿®å¤ä¾èµ–
fix_dependencies() {
    echo "ğŸ”§ æ£€æŸ¥Qwen-VL-Chatä¾èµ–å®Œæ•´æ€§..."
    if ! python -c "
import importlib, sys, subprocess, pkgutil, importlib.util, json, os, warnings, math
warnings.filterwarnings('ignore')

dependencies = {
    'transformers_stream_generator': '0.0.5',
    'tiktoken': '0.7.0',
    'transformers': '4.44.2',
    'torch': '2.2.2',
    'numpy': '1.26.4'
}

missing = []
for pkg, version in dependencies.items():
    try:
        spec = importlib.util.find_spec(pkg)
        if spec is None:
            missing.append(f'{pkg} (æœªå®‰è£…)')
            continue

        module = importlib.import_module(pkg)

        if pkg == 'tiktoken':
            continue

        if hasattr(module, '__version__'):
            module_version = module.__version__
            main_version = version.split('.')[0]
            if not module_version.startswith(main_version):
                missing.append(f'{pkg}=={version} (å½“å‰ç‰ˆæœ¬: {module_version})')
    except Exception as e:
        missing.append(f'{pkg}=={version} (é”™è¯¯: {str(e)})')

if missing:
    print(f'âŒ æ£€æµ‹åˆ°é—®é¢˜: {\", \".join(missing)}')
    sys.exit(1)
else:
    print('âœ… æ‰€æœ‰å¿…éœ€ä¾èµ–éªŒè¯é€šè¿‡')
    sys.exit(0)
" 2>/dev/null; then
        echo "âš ï¸  ä¾èµ–éªŒè¯å¤±è´¥ï¼Œå°è¯•æ¸…ç†å¹¶é‡æ–°å®‰è£…..."

        # æ¸…ç†å¯èƒ½å†²çªçš„åŒ…
        echo "ğŸ§¹ æ¸…ç†å¯èƒ½çš„å†²çªåŒ…..."
        pip uninstall -y transformers_stream_generator tiktoken transformers torch numpy auto-gptq bitsandbytes accelerate

        # é‡æ–°å®‰è£…ä¾èµ–
        install_dependencies
    fi
}

# å®‰è£…ä¾èµ–
install_dependencies() {
    # å‡çº§pip
    echo "ğŸ”§ å‡çº§pip..."
    pip install --upgrade pip setuptools wheel --quiet

    # æ£€æŸ¥GPUæƒ…å†µ
    local gpu_check=$(check_gpu_requirements)
    IFS=':' read -ra gpu_info <<< "$gpu_check"
    local has_gpu="${gpu_info[0]}"
    local cuda_available="${gpu_info[1]}"
    local cuda_version="${gpu_info[2]}"

    # å®‰è£…PyTorch (ä½¿ç”¨Python 3.10å…¼å®¹å‘½ä»¤)
    install_cmd=$(get_pytorch_install_command "$has_gpu" "$cuda_available" "$cuda_version")
    echo "ğŸ”§ å®‰è£…PyTorch 2.2.2..."
    eval "$install_cmd"

    # å®‰è£…å…³é”®ä¾èµ–
    echo "ğŸ”§ å®‰è£…å…³é”®ä¾èµ– (Qwen-VL-Chatå¿…éœ€)..."
    pip install transformers_stream_generator==0.0.5 tiktoken==0.7.0 --upgrade --no-cache-dir --quiet

    # å®‰è£…4-bité‡åŒ–æ”¯æŒ
    echo "ğŸ”§ å®‰è£…4-bité‡åŒ–æ”¯æŒ..."
    pip install auto-gptq==0.7.1 optimum==1.21.0 bitsandbytes==0.44.1 --upgrade --no-cache-dir --quiet

    # å®‰è£…å…¶ä»–ä¾èµ–
    echo "â¬‡ï¸ å®‰è£…å…¶ä»–ä¾èµ–..."
    pip install "transformers==4.44.2" "gradio==4.44.1" "accelerate==1.1.0" --upgrade --quiet
    pip install -r requirements.txt --no-cache-dir --upgrade

    echo "âœ… ä¾èµ–å®‰è£…å®Œæˆ"
}

# éªŒè¯ä¾èµ–
verify_dependencies() {
    python -c "
import importlib, sys, subprocess, pkgutil, importlib.util, json, os, warnings, math
warnings.filterwarnings('ignore')

required_packages = ['transformers_stream_generator', 'tiktoken', 'transformers', 'torch', 'numpy']
missing = []
for pkg in required_packages:
    try:
        importlib.import_module(pkg)
        print(f'âœ… {pkg} åœ¨ç¯å¢ƒä¸­å¯ç”¨')
    except ImportError as e:
        missing.append(pkg)
        print(f'âŒ {pkg} åœ¨ç¯å¢ƒä¸­ä¸å¯ç”¨: {str(e)}')
if missing:
    print(f'âš ï¸  ä»ç„¶ç¼ºå¤±åŒ…: {\", \".join(missing)}')
else:
    print('ğŸ‰ æ‰€æœ‰ä¾èµ–éªŒè¯é€šè¿‡ï¼ŒPython 3.10ç¯å¢ƒå‡†å¤‡å°±ç»ª')
" || echo "âš ï¸  ä¾èµ–éªŒè¯å¤±è´¥ï¼Œä½†å°†ç»§ç»­å¯åŠ¨åº”ç”¨"
}

# æ™ºèƒ½æ£€æŸ¥Qwen-VL-Chatæ¨¡å‹æ–‡ä»¶
check_model_files() {
    local model_dir="qwen_models"

    echo "ğŸ” æ™ºèƒ½æ£€æŸ¥Qwen-VL-Chatæ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§..."

    # æ£€æŸ¥æ¨¡å‹ç›®å½•
    if [ ! -d "$model_dir" ]; then
        echo_qwen_download_prompt "æ¨¡å‹ç›®å½•ä¸å­˜åœ¨"
        return 1
    fi

    # æ£€æŸ¥åŸºç¡€é…ç½®æ–‡ä»¶
    if [ ! -f "$model_dir/config.json" ] || [ ! -f "$model_dir/tokenizer_config.json" ]; then
        echo_qwen_download_prompt "ç¼ºå¤±åŸºç¡€é…ç½®æ–‡ä»¶ (config.json æˆ– tokenizer_config.json)"
        return 1
    fi

    # æ£€æŸ¥Qwenç‰¹å®štokenizeræ–‡ä»¶
    if [ ! -f "$model_dir/qwen.tiktoken" ] || [ ! -f "$model_dir/tokenization_qwen.py" ] || [ ! -f "$model_dir/modeling_qwen.py" ] || [ ! -f "$model_dir/configuration_qwen.py" ]; then
        echo_qwen_download_prompt "ç¼ºå¤±Qwenç‰¹å®šæ–‡ä»¶ (qwen.tiktoken, tokenization_qwen.py, modeling_qwen.py, configuration_qwen.py)"
        return 1
    fi

    # æ£€æŸ¥æƒé‡æ–‡ä»¶ (æ”¯æŒåˆ†ç‰‡)
    local weight_files=()
    local file

    echo "ğŸ” æŸ¥æ‰¾æ‰€æœ‰æƒé‡æ–‡ä»¶..."
    # å®‰å…¨åœ°è·å–æ‰€æœ‰æƒé‡æ–‡ä»¶
    shopt -s nullglob  # ç¡®ä¿æ— åŒ¹é…æ—¶ä¸è¿”å›åŸæ¨¡å¼
    weight_files=("$model_dir"/pytorch_model*.bin)
    shopt -u nullglob

    # è¿‡æ»¤å‡ºçœŸå®æ–‡ä»¶
    local real_files=()
    for file in "${weight_files[@]}"; do
        if [ -f "$file" ]; then
            real_files+=("$file")
        fi
    done

    weight_files=("${real_files[@]}")
    local weight_count=${#weight_files[@]}

    echo "ğŸ” æ‰¾åˆ° ${weight_count} ä¸ªæƒé‡æ–‡ä»¶"

    if [ $weight_count -eq 0 ]; then
        echo_qwen_download_prompt "æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶"
        return 1
    fi

    # æ£€æŸ¥åˆ†ç‰‡æ•°é‡
    local shard_count=0
    echo "ğŸ” æ£€æŸ¥æƒé‡åˆ†ç‰‡æ–‡ä»¶..."

    for file in "${weight_files[@]}"; do
        local filename=$(basename "$file")

        # å®‰å…¨çš„åˆ†ç‰‡æ–‡ä»¶æ£€æµ‹
        if echo "$filename" | grep -q "^pytorch_model-[0-9]\{5\}-of-00010\.bin$"; then
            echo "   [OK] æ‰¾åˆ°åˆ†ç‰‡æ–‡ä»¶: $filename"
            shard_count=$((shard_count + 1))
        else
            echo "   [INFO] éåˆ†ç‰‡æ–‡ä»¶: $filename"
        fi
    done

    echo "ğŸ” æ‰¾åˆ° $shard_count ä¸ªæƒé‡åˆ†ç‰‡æ–‡ä»¶"

    if [ $shard_count -lt 8 ]; then  # å…è®¸ç¼ºå¤±1-2ä¸ªåˆ†ç‰‡
        echo_qwen_download_prompt "ä»…æ‰¾åˆ° $shard_count ä¸ªæƒé‡åˆ†ç‰‡ï¼Œæ¨¡å‹å¯èƒ½ä¸å®Œæ•´ (åº”æœ‰10ä¸ªåˆ†ç‰‡)"
        return 1
    fi

    echo "âœ… Qwen-VL-Chatæ¨¡å‹éªŒè¯æˆåŠŸ!"
    echo "   â€¢ æ‰¾åˆ° ${weight_count} ä¸ªæƒé‡æ–‡ä»¶"
    echo "   â€¢ æ‰¾åˆ° ${shard_count} ä¸ªæƒé‡åˆ†ç‰‡"
    return 0
}

# ä¸‹è½½æç¤ºå‡½æ•°
echo_qwen_download_prompt() {
    local reason="$1"
    echo "âŒ æ¨¡å‹éªŒè¯å¤±è´¥: $reason"
    echo "ğŸ’¡ éœ€è¦ä¸‹è½½å®Œæ•´çš„Qwen-VL-Chatæ¨¡å‹ (çº¦18GB):"
    echo "   python download_models.py"
    echo ""
    read -p "æ˜¯å¦ç°åœ¨ä¸‹è½½æ¨¡å‹? (y/n): " download_model
    if [ "$download_model" = "y" ]; then
        # æ£€æŸ¥ç½‘ç»œè¿æ¥
        if ! ping -c 1 huggingface.co &> /dev/null; then
            echo "âš ï¸  ç½‘ç»œè¿æ¥ä¸ç¨³å®šï¼Œå»ºè®®ä½¿ç”¨é•œåƒæº"
            read -p "ä½¿ç”¨ä¸­å›½é•œåƒæº? (y/n): " use_mirror
            if [ "$use_mirror" = "y" ]; then
                python download_models.py --mirror
            else
                python download_models.py
            fi
        else
            python download_models.py
        fi
    else
        echo "âš ï¸  è¯·å…ˆä¸‹è½½å®Œæ•´æ¨¡å‹å†è¿è¡Œä¸»ç¨‹åº"
        exit 1
    fi
}

# ä¸»æµç¨‹
main() {
    # 1. æ£€æŸ¥ç£ç›˜ç©ºé—´
    check_disk_space

    # 2. è®¾ç½®è™šæ‹Ÿç¯å¢ƒ
    setup_virtual_env

    # 3. æ£€æŸ¥æ¨¡å‹
    #check_model_files

    # 4. å¯åŠ¨åº”ç”¨
    echo "ğŸ¯ å¯åŠ¨åº”ç”¨ (è®¿é—® http://127.0.0.1:9527)..."
    echo ""

    # ä¼ é€’æ‰€æœ‰å‚æ•°ç»™app.py
    python app.py "$@"

    echo ""
    echo "ğŸ‘‹ åº”ç”¨å·²å…³é—­"
}
# æ‰§è¡Œä¸»æµç¨‹
main "$@"