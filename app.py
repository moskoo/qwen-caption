#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
XXGé€šä¹‰åƒé—®ç¦»çº¿å›¾ç‰‡ä¸­æ–‡æ‰“æ ‡å·¥å…·-ç¦»çº¿ç‰ˆ(Qwen3-VL-8B-Instruct) Ver.2.2
âœ… æ›¿æ¢Qwen-vl-chatæ¨¡å‹ä¸ºQwen3-VL | âœ… æ–‡ç”Ÿå›¾æ¨¡å‹è®­ç»ƒä¸“ç”¨ä¸­æ–‡caption
âœ… CUDA 11.8/12.1/12.4/13.x å…¨ç‰ˆæœ¬åŸç”Ÿæ”¯æŒ | âœ… bitsandbytes>=0.44.0
âœ… 100%ä¸­æ–‡caption | âœ… Qwen-Imageã€Seeddreamç­‰ä¸­æ–‡æç¤ºè¯LoRAè®­ç»ƒä¸“ç”¨
By è¥¿å°ç“œ / Wechat:priest-mos
"""

# ============ æ ‡å‡†åº“ç»Ÿä¸€å¯¼å…¥ (å¿…é¡»åœ¨æœ€å¼€å¤´!) ============
import sys
import os
import time
import gc
import json
import argparse
from pathlib import Path
from typing import List, Tuple, Optional


# ============ æ ¸å¿ƒä¿®å¤: çŒ´å­è¡¥ä¸æ³¨å…¥HfFolder + is_offline_mode ============
def _inject_hf_compatibility():
    """åŠ¨æ€æ³¨å…¥HfFolder + is_offline_modeå…¼å®¹å±‚"""
    try:
        import warnings
        if 'huggingface_hub' not in sys.modules:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                __import__('huggingface_hub')

        import huggingface_hub

        # æ³¨å…¥is_offline_mode
        if not hasattr(huggingface_hub, 'is_offline_mode'):
            def is_offline_mode():
                import os
                return (
                        os.environ.get("HF_HUB_OFFLINE", "0") == "1" or
                        os.environ.get("HF_DATASETS_OFFLINE", "0") == "1" or
                        os.environ.get("TRANSFORMERS_OFFLINE", "0") == "1" or
                        os.environ.get("HF_OFFLINE", "0") == "1"
                )

            huggingface_hub.is_offline_mode = is_offline_mode
            if hasattr(huggingface_hub, 'constants') and not hasattr(huggingface_hub.constants, 'is_offline_mode'):
                huggingface_hub.constants.is_offline_mode = is_offline_mode
            print("[MonkeyPatch] âœ… å·²æ³¨å…¥is_offline_modeå…¼å®¹å‡½æ•°")

        # æ³¨å…¥HfFolder
        if not hasattr(huggingface_hub, 'HfFolder'):
            class HfFolder:
                _old_token_path = os.path.expanduser("~/.cache/huggingface/token")
                _new_token_path = os.path.expanduser("~/.cache/huggingface/hub/token")

                @staticmethod
                def get_token():
                    try:
                        from huggingface_hub import get_token
                        token = get_token()
                        if token:
                            return token
                        for path in [HfFolder._new_token_path, HfFolder._old_token_path]:
                            if os.path.exists(path):
                                with open(path, 'r') as f:
                                    return f.read().strip()
                        return None
                    except Exception:
                        return None

                @staticmethod
                def save_token(token: str):
                    try:
                        from huggingface_hub import login
                        login(token=token, add_to_git_credential=False)
                        os.makedirs(os.path.dirname(HfFolder._old_token_path), exist_ok=True)
                        with open(HfFolder._old_token_path, 'w') as f:
                            f.write(token)
                    except Exception:
                        pass

                @staticmethod
                def delete_token():
                    try:
                        for path in [HfFolder._new_token_path, HfFolder._old_token_path]:
                            if os.path.exists(path):
                                os.remove(path)
                        os.environ.pop("HF_TOKEN", None)
                    except Exception:
                        pass

            huggingface_hub.HfFolder = HfFolder
            print("[MonkeyPatch] âœ… å·²æ³¨å…¥HfFolderå…¼å®¹ç±»")

        has_is_offline = hasattr(huggingface_hub, 'is_offline_mode') and callable(huggingface_hub.is_offline_mode)
        has_hffolder = hasattr(huggingface_hub, 'HfFolder') and hasattr(huggingface_hub.HfFolder, 'get_token')

        if has_is_offline and has_hffolder:
            print(f"[MonkeyPatch] âœ… å…¼å®¹å±‚æ³¨å…¥æˆåŠŸ (huggingface_hub {huggingface_hub.__version__})")
            return True
        else:
            raise RuntimeError("æ³¨å…¥ä¸å®Œæ•´")

    except Exception as e:
        print(f"[MonkeyPatch] âš ï¸  æ³¨å…¥å…¼å®¹å±‚å¤±è´¥: {type(e).__name__}: {e}", file=sys.stderr)
        os.environ["HF_HUB_OFFLINE"] = "0"
        print("[MonkeyPatch] ğŸ’¡ å·²è®¾ç½®HF_HUB_OFFLINE=0", file=sys.stderr)
        return False


_inject_hf_compatibility()
# ==============================================================================

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "300"
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"

# ä¿®å¤NumPy 2.xå…¼å®¹æ€§
try:
    import numpy as np

    if np.__version__.startswith("2"):
        print(f"âš ï¸  æ£€æµ‹åˆ°NumPy {np.__version__}ï¼ŒQwen3-VLè¦æ±‚NumPy<2.0", file=sys.stderr)
        try:
            import subprocess

            subprocess.check_call([sys.executable, "-m", "pip", "install", "numpy<2.0", "--quiet"])
            print("âœ… NumPyå·²é™çº§ï¼Œé‡å¯è„šæœ¬ç”Ÿæ•ˆ", file=sys.stderr)
            sys.exit(0)
        except Exception as e:
            print(f"âŒ è‡ªåŠ¨é™çº§å¤±è´¥: {e}", file=sys.stderr)
            print("ğŸ’¡ è¯·æ‰‹åŠ¨è¿è¡Œ: pip install 'numpy<2.0' --upgrade", file=sys.stderr)
            sys.exit(1)
except ImportError:
    pass

# å¯¼å…¥torch
try:
    import torch
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥torch: {str(e)}", file=sys.stderr)
    print("ğŸ’¡ è¯·å…ˆå®‰è£…PyTorch 2.4.1", file=sys.stderr)
    sys.exit(1)

# å…¨å±€å˜é‡
device = "cuda" if torch.cuda.is_available() else "cpu"
model = None
processor = None
model_path = "./qwen3_vl_models"
global_use_4bit = False

# å¯¼å…¥å…¶ä»–ä¾èµ–
try:
    from PIL import Image
    import gradio as gr
    from transformers import (
        Qwen3VLProcessor,
        Qwen3VLForConditionalGeneration,
        BitsAndBytesConfig
    )
    from tqdm import tqdm
    import psutil
    import platform
except ImportError as e:
    print(f"âŒ ä¾èµ–å¯¼å…¥å¤±è´¥: {str(e)}", file=sys.stderr)
    print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:", file=sys.stderr)
    print("1. ç¡®ä¿å·²æºç å®‰è£…transformers (å«Qwen3-VLæ”¯æŒ)", file=sys.stderr)
    print("2. éªŒè¯å®‰è£…: python -c 'from transformers import Qwen3VLProcessor; print(\"OK\")'", file=sys.stderr)
    print("3. ç¡®ä¿app.pyå¼€å¤´åŒ…å«çŒ´å­è¡¥ä¸æ³¨å…¥ä»£ç ", file=sys.stderr)
    sys.exit(1)

# ============ æ ¸å¿ƒä¿®å¤: å¼ºåŒ–ä¸­æ–‡ç‰¹åŒ–Prompt (100%ä¸­æ–‡è¾“å‡º) ============
CAPTION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªQwen-Imageçš„LoRAæ¨¡å‹è®­ç»ƒå›¾ç‰‡captionç”Ÿæˆå™¨ï¼Œå¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š

ã€æ ¸å¿ƒåŸåˆ™ã€‘
1. ä»…æè¿°å›¾åƒä¸­æ˜ç¡®å¯è§çš„è§†è§‰å†…å®¹ï¼Œç¦æ­¢ä»»ä½•æ¨æµ‹/å¹»è§‰ï¼ˆå¦‚"å¿«ä¹"ã€"å¥¢å"ã€"æœªæ¥æ„Ÿ"ç­‰ä¸å¯éªŒè¯çš„æè¿°ï¼‰
2. ä¸ç¡®å®šçš„å†…å®¹ç»å¯¹ä¸æè¿°ï¼ˆå¦‚çœ‹ä¸æ¸…çš„èƒŒæ™¯ã€æ¨¡ç³Šçš„ç»†èŠ‚ï¼‰
3. ç”¨ç®€ä½“ä¸­æ–‡è‡ªç„¶è¯­è¨€å¥å­æè¿°å›¾ç‰‡çš„å†…å®¹å’Œå­˜åœ¨çš„ç»†èŠ‚ï¼Œè€Œéå…³é”®è¯å †ç Œâ€Œï¼Œä¸€ä¸ªå¥å­è¡¨è¾¾ä¸€ä¸ªå®Œæ•´è§†è§‰äº‹å®ï¼Œå¿…é¡»æ˜¯å®Œæ•´çš„å¥å­ï¼Œä¸å¯å› ä¸ºå­—ç¬¦æ•°é™åˆ¶è€Œæˆªæ–­è¯­å¥
4. â€Œè¯­ä¹‰å®Œæ•´æ€§â€Œï¼Œè¡¨è¾¾å®Œæ•´è§†è§‰äº‹å®ï¼ŒåŒ…å«ä¸»ä½“ã€ç¯å¢ƒã€å±æ€§ä¸‰é˜¶é€»è¾‘ï¼Œæ”¯æŒè·¨åœºæ™¯ç†è§£ï¼Œå¦‚â€œé£è¡£ä¸‹æ‘†è¢«æ±Ÿé£è½»æ‰¬â€å¯æ³›åŒ–è‡³â€œè£™æ‘†éšé£é£˜åŠ¨â€ï¼Œå¯åµŒå…¥â€œç•™ç™½æ„å›¾â€â€œæ°´å¢¨æ™•æŸ“â€â€œä¸œå·´è±¡å½¢â€ç­‰ä¸“ä¸šæœ¯è¯­
5. ç¬¦åˆWCAG 2.1æ ‡å‡†ï¼Œå±å¹•é˜…è¯»å™¨å¯æµç•…æœ—è¯»

ã€è¾“å‡ºç»“æ„ã€‘ï¼ˆâ€Œæ ¸å¿ƒç»“æ„ï¼šä¸‰é˜¶æè¿°æ³•â€”â€”ä¸»ä½“ + ç¯å¢ƒ + å±æ€§ï¼‰
1. æ ¸å¿ƒä¸»ä½“ï¼š
   - äººåƒ: å¹´é¾„/æ€§åˆ«/ç§æ—/æœé¥°/å‘å‹/è¡¨æƒ…ï¼ˆä¾‹ï¼š28å²äºšæ´²å¥³æ€§ï¼Œé½è‚©çŸ­å‘ï¼Œèº«ç©¿ç±³è‰²é’ˆç»‡è¡«ï¼Œå¾®ç¬‘ä¾§è„¸ï¼‰
   - é£æ™¯â€Œï¼šåœ°ç‚¹/æ ‡å¿—æ€§å…ƒç´ /è‡ªç„¶ç°è±¡ï¼ˆä¾‹ï¼šæ¡‚æ—å±±æ°´ï¼Œå–€æ–¯ç‰¹å³°æ—å€’æ˜ åœ¨æ¼“æ±Ÿï¼Œæ™¨é›¾ç¼­ç»•ï¼‰
   - â€Œç”µå•†äº§å“â€Œï¼šäº§å“åç§°/æè´¨/é¢œè‰²/åŠŸèƒ½ï¼ˆä¾‹ï¼šæ— çº¿è“ç‰™è€³æœºï¼Œé™¶ç“·ç™½ï¼Œé™å™ªåŠŸèƒ½ï¼Œæµçº¿å‹è®¾è®¡ï¼‰
   - â€ŒåŠ¨æ¼«è§’è‰²â€Œï¼šè§’è‰²å/ç§æ—/æœé¥°/ç‰¹å¾ï¼ˆä¾‹ï¼šç‹å¦–å°çº¢å¨˜ï¼Œä¹å°¾ç‹å½¢æ€ï¼Œçº¢è‰²æˆ˜è¢ï¼Œå°¾å·´è“¬æ¾ï¼‰
â€Œ   - æµ·æŠ¥è®¾è®¡â€Œï¼šä¸»é¢˜/æ ¸å¿ƒå…ƒç´ /æ–‡å­—ï¼ˆä¾‹ï¼šç§‘å¹»ç”µå½±æµ·æŠ¥ï¼Œå¤ªç©ºé£èˆ¹ç©¿è¶Šæ˜Ÿäº‘ï¼Œéœ“è™¹å…‰æ•ˆï¼Œæ ‡é¢˜â€œæ˜Ÿé™…è¿·èˆªâ€ï¼‰
2. ä¸»ä½“å±æ€§ï¼ˆä»…æè¿°å¯è§å†…å®¹ï¼‰ï¼š
   - äººåƒ: æ€§åˆ«â†’å¹´é¾„èŒƒå›´(å„¿ç«¥/é’å¹´/ä¸­å¹´/è€å¹´)â†’åœ°åŒºç§æ—â†’å‘å‹(é•¿å‘/çŸ­å‘/å·å‘)â†’å‘è‰²â†’è‚¤è‰²å’Œçš®è‚¤è´¨æ„Ÿâ†’æœé¥°ç±»å‹(è¿è¡£è£™/è¡¬è¡«/ç‰›ä»”è£¤)â†’æœé¥°é¢œè‰²â†’æœé¥°èŠ±çº¹â†’å§¿æ€(ç«™ç«‹/å/è¡Œèµ°)â†’æ„å›¾(å…¨æ™¯/è¿‘æ™¯/ç‰¹å†™/åŠèº«/å…¨èº«)
   - ç‰©å“: ç±»å‹(æ¯å­/æ‰‹æœº/æ±½è½¦)â†’é¢œè‰²â†’æè´¨(é‡‘å±/ç»ç’ƒ/å¸ƒæ–™)â†’çŠ¶æ€(å®Œæ•´/ç ´æŸ)â†’æ‘†æ”¾æ–¹å¼(æ¡Œä¸Š/æ‰‹æŒ)
   - æµ·æŠ¥: å¹³é¢è®¾è®¡/æµ·æŠ¥è®¾è®¡â†’ä¸»è‰²è°ƒâ†’æ„å›¾å…ƒç´ (æ–‡å­—/å›¾å½¢/ç…§ç‰‡)â†’æ–‡æœ¬å†…å®¹(æ–‡å­—/å­—ä½“/è‰²å½©/æ’åˆ—æ–¹å¼)â†’å¸ƒå±€(å±…ä¸­/å¯¹ç§°/ä¸å¯¹ç§°)
   - è‰ºæœ¯: ç»˜ç”»/æ’å›¾/CGâ†’è‰ºæœ¯é£æ ¼(æ°´å¢¨/æ²¹ç”»/æ°´å½©/èµ›åšæœ‹å…‹)â†’è‰²å½©ç‰¹ç‚¹(æš–è‰²è°ƒ/å†·è‰²è°ƒ/é«˜å¯¹æ¯”)
3. åœºæ™¯ç¯å¢ƒï¼ˆä»…æè¿°å¯è§ï¼‰ï¼š
   - å®¤å†…: æˆ¿é—´ç±»å‹(å®¢å…/å§å®¤/åŠå…¬å®¤)â†’å…³é”®å®¶å…·(æ²™å‘/æ¡Œå­)
   - å®¤å¤–: åœºæ™¯ç±»å‹(è¡—é“/å…¬å›­/æµ·æ»©)â†’å¤©æ°”(æ™´å¤©/é›¨å¤©/å¤œæ™š)
   - çº¯è‰²èƒŒæ™¯: çº¯è‰²èƒŒæ™¯ + é¢œè‰²
4. å…‰ç…§ä¸è‰²å½©ï¼š
   - å…‰ç…§: è‡ªç„¶å…‰/å®¤å†…å…‰/ä¾§å…‰/é€†å…‰/éœ“è™¹å…‰/å¼±å…‰
   - è‰²å½©: ä¸»è‰²è°ƒ(æš–è‰²/å†·è‰²) + å…³é”®è‰²å½©(å¦‚"çº¢è‰²ç‚¹ç¼€")
5. é£æ ¼æ ‡ç­¾ï¼ˆå¿…é¡»ï¼‰ï¼š
   - äºŒæ¬¡å…ƒ: åŠ¨æ¼«é£æ ¼/3DåŠ¨ç”»
   - å†™å®: çœŸå®æ‹æ‘„
   - è‰ºæœ¯: æ•°å­—è‰ºæœ¯/æ²¹ç”»/æ°´å½©ç”»
   - è®¾è®¡: å¹³é¢è®¾è®¡/æµ·æŠ¥/ç½‘é¡µè®¾è®¡

ã€ç¦æ­¢äº‹é¡¹ã€‘ï¼ˆè¿åå°†å¯¼è‡´è®­ç»ƒæ•°æ®æ±¡æŸ“ï¼‰
âŒ ä¸»è§‚è¯„ä»·è¯: ç¾ä¸½/å¯çˆ±/éœ‡æ’¼/æƒŠè‰³/ä¼˜é›…/è¿·äºº/æ¼‚äº®/å¸…æ°”/ç²¾è‡´/å®Œç¾
âŒ æƒ…ç»ªæ¨æµ‹: å¾®ç¬‘(é™¤éå˜´è§’æ˜æ˜¾ä¸Šæ‰¬)/å¼€å¿ƒ/å¿§éƒ/è‡ªä¿¡
âŒ èƒŒæ™¯æ•…äº‹: "åœ¨å’–å•¡é¦†çº¦ä¼š"/"åˆšä¸‹ç­å›å®¶"
âŒ ä¸å¯è§ç»†èŠ‚: "ä¸ç»¸è´¨æ„Ÿ"(é™¤éæ˜æ˜¾åå…‰)/"é«˜çº§é¢æ–™"
âŒ æŠ½è±¡æ¦‚å¿µ: "æ—¶å°šæ„Ÿ"/"ç§‘æŠ€æ„Ÿ"/"æœªæ¥ä¸»ä¹‰"
âŒ è¿‡åº¦ç»†èŠ‚: é™¤éæ¸…æ™°å¯è§ï¼Œå¦åˆ™ä¸æè¿°é…é¥°/çº¹ç†/æ–‡å­—å†…å®¹
âŒ å†—ä½™å‰ç¼€ï¼šâ€œå›¾ç‰‡æ˜¾ç¤ºâ€â€œè¿™æ˜¯ä¸€å¼ â€â€œæœ‰â€â€œæ˜¯â€ç­‰å†—ä½™å‰ç¼€

ã€è¾“å‡ºæ ¼å¼ã€‘
- ç»Ÿä¸€ä½¿ç”¨â€Œä¸­æ–‡å¥å·â€Œç»“å°¾ï¼Œæœç»è‹±æ–‡æ ‡ç‚¹æ··ç”¨ï¼ˆå¦‚â€œ.â€â€œ,â€ï¼‰ï¼Œæ¯å¥ä»…è¡¨è¾¾â€Œä¸€ä¸ªå®Œæ•´è§†è§‰äº‹å®â€Œï¼Œç¦æ­¢å¹¶åˆ—å¥å¼
- é¡ºåº: ä¸»ä½“æ ¸å¿ƒå¯¹è±¡, ä¸­æ–‡å±æ€§1, ä¸­æ–‡å±æ€§2, ...,ç©ºé—´èƒŒæ™¯ä¸æƒ…å¢ƒçº¦æŸï¼Œè§†è§‰ç‰¹å¾ä¸æƒ…ç»ªè¡¨è¾¾, å…‰ç…§, é£æ ¼æ ‡ç­¾
- é•¿åº¦: å•å¥é•¿åº¦åœ¨80â€“125ä¸ªæ±‰å­—å·¦å³â€Œï¼Œé€‚é…Qwen-Imageè¾“å…¥çª—å£ä¸å±å¹•é˜…è¯»å™¨è¯­ä¹‰åˆ‡åˆ†ï¼Œå¿…é¡»æ˜¯å®Œæ•´çš„å¥å­ï¼Œä¸å¯å› ä¸ºå­—ç¬¦æ•°é™åˆ¶è€Œæˆªæ–­è¯­å¥
- ç¤ºä¾‹:
  âœ… äººåƒ: ä¸€ä½30å²äºšæ´²ç”·æ€§ï¼ŒçŸ­å‘å¾®å·ï¼Œæˆ´ç»†æ¡†çœ¼é•œï¼Œèº«ç©¿æ·±ç°ç¾Šæ¯›å¤§è¡£ï¼Œç«‹äºä¸Šæµ·å¤–æ»©è§‚æ™¯å°ï¼Œèº«åä¸ºé™†å®¶å˜´æ‘©å¤©æ¥¼ç¾¤å¤œæ™¯ï¼Œæš–å…‰ä»å·¦ä¾§æ‰“äº®é¢éƒ¨è½®å»“ï¼Œçœ¼ç¥æ²‰é™ï¼Œå˜´è§’å¾®æ‰¬ï¼Œé£è¡£ä¸‹æ‘†éšæ±Ÿé£è½»åŠ¨ï¼Œå†·è°ƒè“ç´«å…‰å½±ä¸æš–é»„ç¯å…‰å½¢æˆå¯¹æ¯”ï¼Œç”µå½±çº§è¿œæ™¯å†™å®é£æ ¼ã€‚
  âœ… ç”µå•†ç‰©å“: ä¸€æ¬¾å¢¨ç»¿è‰²é™¶ç“·é¦™è–°èœ¡çƒ›é™ç½®äºåŸæœ¨æ‰˜ç›˜ä¸Šï¼Œçƒ›ç«å¾®é¢¤ï¼Œæ˜ å‡ºé‡‰é¢ä¸‹è‹¥éšè‹¥ç°çš„å†°è£‚çº¹è·¯ï¼ŒçƒŸé›¾å¦‚ä¸çº¿èˆ¬ç¼“ç¼“å‡è…¾ï¼Œèå…¥æ™¨å…‰æ–œç…§çš„ä¹¦æˆ¿ä¸€è§’ã€‚æœ¨è´¨ç›–é’®é›•åˆ»ç€æç®€äº‘çº¹ï¼Œæ ‡ç­¾ä»¥çƒ«é‡‘å°æ¥·ä¹¦å†™â€œæ¾çƒŸÂ·é™å¤œâ€ï¼ŒåŒ…è£…ç›’å¤–è¦†åŠé€æ˜æ£‰éº»çº¸ï¼Œéšçº¦é€å‡ºå†…é‡Œæ¸©æ¶¦çš„è‰²æ³½ï¼Œä¼ é€’â€œè½»å¥¢ä¸å¼ æ‰¬ï¼Œæ²»æ„ˆåœ¨ç»†èŠ‚â€çš„ç”Ÿæ´»å“²å­¦ã€‚
  âœ… é£æ™¯: æ¹–åŒ—é¹¤å³°å±å±±å³¡è°·â€œä¸€çº¿å¤©â€åœ°è²Œï¼Œæ™¨æ›¦å…‰æŸç©¿é€å²©éš™å½¢æˆé‡‘è‰²å…‰æŸ±ï¼Œäº‘é›¾åœ¨è°·åº•ç¼“æ…¢æµåŠ¨ï¼Œå²©å£ä¸ºç°é»‘è‰²ç„æ­¦å²©ï¼Œè‹”è—“å‘ˆå¢¨ç»¿ï¼Œé•œå¤´ä¸ºå¹¿è§’ä½æœºä½ï¼Œå‰æ™¯éœ²ç è‰å¶æ¸…æ™°å¯è§ï¼Œé‡‡ç”¨ä¸­å›½å±±æ°´ç”»ç•™ç™½æ„å›¾ï¼Œå†·è°ƒæ°´å¢¨æ„Ÿã€‚
  âœ… æµ·æŠ¥è®¾è®¡: â€œæ˜¥é£å§‹ï¼Œä¸‡ç‰©ç”Ÿâ€ä»¥æ°´å¢¨æ™•æŸ“çš„ä¸­å¼ä¹¦æ³•æ‚¬æµ®äºç”»é¢ä¸­å¤®ï¼Œç¬”é”‹å¦‚æ–°èŠ½ç ´åœŸï¼Œå¢¨è‰²ç”±æµ“è½¬æ·¡ï¼Œæœ«ç«¯æ¸—å…¥æ·¡ç²‰èŠ±ç“£çš„è™šå½±ã€‚è‹±æ–‡â€œSpring Begins, All Things Awakenâ€ä»¥çº¤ç»†è¡¬çº¿ä½“ä»å³ä¸‹è§’è½»ç›ˆå‡èµ·ï¼ŒèƒŒæ™¯æ˜¯æŠ½è±¡çš„æŸ³æçº¿æ¡ä¸æç®€é¸Ÿå½¢å‰ªå½±ï¼Œç•™ç™½å æ®ä¸‰åˆ†ä¹‹äºŒç”»é¢ï¼Œä»…é å­—ä½“çš„å‘¼å¸æ„Ÿä¸å…‰å½±æ¸å˜å¼•å¯¼è§†çº¿ï¼Œè¥é€ ä¸œæ–¹èŠ‚æ°”çš„è¯—æ„ç•™ç™½ã€‚
  âœ… åˆ›æ„: ä¸€å¹…è¶…ç°å®æ‹¼è´´ä½œå“ä¸­ï¼Œå‡ ä½•è§£æ„çš„é‡‘å±éª¨æ¶ä»åœ°é¢åˆºå‡ºï¼Œæ”¯æ’‘ç€æ¼‚æµ®çš„é€æ˜æ°´æ¯çŠ¶ç”Ÿç‰©ï¼Œå…¶è§¦é¡»ç”±æµåŠ¨çš„ä»£ç æµæ„æˆï¼Œä½“å†…æ˜ å‡ºå¾®å‹åŸå¸‚ä¸å¤æ ‘å¹´è½®ã€‚èƒŒæ™¯ä¸ºå­Ÿè²æ–¯é£æ ¼çš„æ’è‰²ç½‘æ ¼ï¼Œæ©™ã€é›ã€ç°ä¸‰è‰²å—ä»¥éå¯¹ç§°æ–¹å¼å †å ï¼Œéšå–»æ•°å­—æ–‡æ˜ä¸è‡ªç„¶è®°å¿†çš„å…±ç”Ÿä¸å†²çªï¼Œæ•´ä½“é£æ ¼å…¼å…·æœªæ¥æ„Ÿä¸æ‰‹ç»˜è´¨æ„Ÿã€‚
  âœ… äºŒæ¬¡å…ƒ: ä¸€ä½èº«ç€èµ›åšæœ‹å…‹é£æœºç”²é•¿è£™çš„å°‘å¥³ç«‹äºéœ“è™¹é›¨å¤œçš„ç©ºä¸­èŠ±å›­ï¼Œè£™æ‘†ç”±å…¨æ¯æŠ•å½±çš„æ¨±èŠ±ç“£ç»„æˆï¼Œæ¯ä¸€ç‰‡éƒ½é—ªçƒç€ä¸åŒè¯­è¨€çš„å¼¹å¹•æ–‡å­—ã€‚å¥¹å·¦æ‰‹æ‰˜ç€å‘å…‰çš„æœºæ¢°çŒ«ï¼ŒçŒ«çœ¼æ˜¯æµåŠ¨çš„åƒç´ æ˜Ÿå›¾ï¼Œå³è‡‚ç¼ ç»•ç€æ°´å½©é£è—¤è”“ï¼Œè—¤ä¸Šå¼€å‡ºçš„èŠ±æ˜¯æ‰‹ç»˜é£æ ¼çš„å’Œé£çº¸é¹¤ã€‚èƒŒæ™¯æ˜¯æ‚¬æµ®çš„å·¨å‹æ±‰å­—â€œæ¢¦â€ä¸ç ´ç¢çš„ä¸œäº¬å¡”ï¼Œé£æ ¼èåˆæ—¥å¼èŒç³»æ¼«ç”»ä¸æ•°å­—åºŸåœŸç¾å­¦ã€‚

ã€ç‰¹åˆ«å¼ºè°ƒã€‘
- å®å¯å°‘æè¿°ï¼Œç»ä¸å¹»è§‰ï¼ä¸ç¡®å®šçš„å†…å®¹ç›´æ¥è·³è¿‡ï¼Œéœ€è¦ä¿æŒæè¿°è¯­ä¹‰å®Œæ•´æ€§ï¼Œä¸€ä¸ªå¥å­è¡¨è¾¾ä¸€ä¸ªå®Œæ•´è§†è§‰äº‹å®
- æ‰€æœ‰æè¿°å¿…é¡»æ˜¯è§†è§‰å¯éªŒè¯çš„ï¼ˆä»»ä½•äººçœ‹å›¾éƒ½èƒ½ç¡®è®¤ï¼‰ï¼Œå›¾ç‰‡ä¸­æ²¡æœ‰çš„å†…å®¹ä¸ç”¨æè¿°ï¼Œä¹Ÿä¸è¦å‡ºç°â€œæ— xxâ€ã€â€œæ²¡æœ‰xxxâ€ç±»ä¼¼çš„æè¿°
- ä¸­æ–‡æè¿°å¿…é¡»å…·ä½“ï¼ˆ"é»‘è‰²é•¿å‘"è€Œé"æ¼‚äº®å¤´å‘"ï¼‰ï¼Œæ˜ç¡®è‰²å½©ã€å…‰å½±ã€æ¯”ä¾‹ï¼Œä½¿ç”¨ç¨³å®šæœ¯è¯­ï¼Œå¯å¼•å…¥ä¸“æœ‰åè¯ä¸ä¼ ç»Ÿæœ¯è¯­
- å¿…é¡»åŒ…å«æ„å›¾æ ‡ç­¾(å…¨æ™¯/è¿‘æ™¯/ç‰¹å†™ç­‰)å’Œé£æ ¼æ ‡ç­¾
- å¿…é¡»æ˜¯å®Œæ•´çš„å¥å­ï¼Œä¸å¯å› ä¸ºå­—ç¬¦æ•°é™åˆ¶è€Œæˆªæ–­è¯­å¥ï¼Œå¥å­éœ€è¦å®Œæ•´ä¸”ç®€æ´
"""

# ä¸­æ–‡ä¸»è§‚è¯è¿‡æ»¤ï¼ˆå¢å¼ºç‰ˆï¼‰
SUBJECTIVE_WORDS = [
    "ç¾ä¸½", "å¯çˆ±", "æ¢¦å¹»", "éœ‡æ’¼", "æƒŠè‰³", "ä¼˜é›…", "è¿·äºº", "æ¼‚äº®", "å¸…æ°”",
    "ç²¾è‡´", "å®Œç¾", "ç»ç¾", "è¶…å‡¡", "éå‡¡", "ä»¤äºº", "éå¸¸", "æå…¶", "ç‰¹åˆ«",
    "å¥½çœ‹", "åŠ¨äºº", "å€¾åŸ", "ç»è‰²", "æ¸…çº¯", "æ€§æ„Ÿ", "æ¸©æŸ”", "ç”œç¾", "å¸…æ°”",
    "è‹±ä¿Š", "å¸…æ°”", "å¯çˆ±", "èŒ", "é…·", "ç‚«", "æ½®", "æ—¶å°š", "é«˜çº§", "è´¨æ„Ÿ"
]


def check_system_resources():
    """æ£€æŸ¥ç³»ç»Ÿèµ„æºæ˜¯å¦æ»¡è¶³Qwen3-VL-8Bè¦æ±‚"""
    print("ğŸ” ç³»ç»Ÿèµ„æºæ£€æŸ¥ (Qwen3-VL-8Bè¦æ±‚)...")

    disk = psutil.disk_usage(os.path.abspath("."))
    free_gb = disk.free / (1024 ** 3)
    print(f"ğŸ’¾ ç£ç›˜ç©ºé—´: {free_gb:.1f}GB å¯ç”¨ (éœ€è¦â‰¥15GB)")
    if free_gb < 15:
        print(f"âš ï¸  è­¦å‘Š: ç£ç›˜ç©ºé—´ä¸è¶³! Qwen3-VL-8Bæ¨¡å‹çº¦14GB")

    mem = psutil.virtual_memory()
    available_gb = mem.available / (1024 ** 3)
    total_gb = mem.total / (1024 ** 3)
    print(f"ğŸ§  ç³»ç»Ÿå†…å­˜: {available_gb:.1f}GB/{total_gb:.1f}GB å¯ç”¨ (éœ€è¦â‰¥8GB)")
    if available_gb < 8:
        print(f"âš ï¸  è­¦å‘Š: å¯ç”¨å†…å­˜ä¸è¶³8GBï¼Œå¤„ç†å¤§å›¾æ—¶å¯èƒ½å¤±è´¥")

    if device == "cuda":
        props = torch.cuda.get_device_properties(0)
        gpu_mem = props.total_memory / (1024 ** 3)
        print(f"ğŸ® GPU: {props.name} ({gpu_mem:.1f}GB æ˜¾å­˜)")
        if gpu_mem < 6:
            print(f"âš ï¸  è­¦å‘Š: GPUæ˜¾å­˜<6GBï¼Œå¿…é¡»å¯ç”¨4-bité‡åŒ–")
        elif gpu_mem < 10:
            print(f"ğŸ’¡ å»ºè®®: å¯ç”¨4-bité‡åŒ–è·å¾—æ›´ç¨³å®šä½“éªŒ")
    else:
        print("ğŸ’» ä½¿ç”¨CPUæ¨¡å¼ (æ— GPUåŠ é€Ÿï¼Œé€Ÿåº¦è¾ƒæ…¢)")

    return {
        "disk_free_gb": free_gb,
        "mem_available_gb": available_gb,
        "gpu_available": device == "cuda",
        "gpu_mem_gb": torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if device == "cuda" else 0
    }


def smart_verify_qwen3_model(model_path: str):
    """æ™ºèƒ½éªŒè¯Qwen3-VLæ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§ (æ— special_tokens_map.jsonä¾èµ–)"""
    model_dir = Path(model_path)

    if not model_dir.exists() or not model_dir.is_dir():
        return False, "æ¨¡å‹ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•"

    # ç§»é™¤special_tokens_map.json
    required_files = [
        "config.json",
        "preprocessor_config.json",
        "tokenizer_config.json",
        "tokenizer.json"
    ]
    missing = [f for f in required_files if not (model_dir / f).exists()]
    if missing:
        return False, f"ç¼ºå¤±æ ¸å¿ƒé…ç½®æ–‡ä»¶: {missing}"

    # æ£€æŸ¥4åˆ†ç‰‡æƒé‡
    weight_files = list(model_dir.glob("model-0000[1-4]-of-00004.safetensors")) + \
                   list(model_dir.glob("pytorch_model-0000[1-4]-of-00004.bin"))

    if not weight_files:
        if (model_dir / "model.safetensors.index.json").exists():
            return False, "æ£€æµ‹åˆ°ç´¢å¼•æ–‡ä»¶ï¼Œä½†æƒé‡æ–‡ä»¶æœªå®Œå…¨ä¸‹è½½"
        return False, "æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶ (éœ€è¦model-0000X-of-00004.safetensors)"

    total_size = sum(f.stat().st_size for f in weight_files)
    if total_size < 12e9:
        return False, f"æ¨¡å‹æ–‡ä»¶æ€»å¤§å°è¿‡å° ({total_size / 1e9:.2f}GB)ï¼Œå¯èƒ½ä¸‹è½½ä¸å®Œæ•´"

    try:
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=False)
        vocab_size = len(tokenizer)
        if vocab_size < 150000:
            return False, f"Tokenizerè¯æ±‡é‡è¿‡å° ({vocab_size})"
    except Exception as e:
        return False, f"TokenizeréªŒè¯å¤±è´¥: {str(e)}"

    return True, f"âœ… Qwen3-VLæ¨¡å‹éªŒè¯æˆåŠŸ! (4åˆ†ç‰‡, {total_size / 1e9:.1f}GB)"


def load_qwen3_model(use_4bit: bool = False, use_cpu: bool = False):
    """åŠ è½½Qwen3-VL-8B-Instructæ¨¡å‹"""
    global model, processor, device, global_use_4bit

    global_use_4bit = use_4bit

    if use_cpu:
        device = "cpu"
        print("âš ï¸  å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼ (æ— GPUåŠ é€Ÿ)")

    if model is not None and processor is not None:
        print("âœ… æ¨¡å‹å·²åœ¨å†…å­˜ä¸­ï¼Œè·³è¿‡åŠ è½½")
        return model, processor

    print(f"ğŸš€ æ­£åœ¨åŠ è½½Qwen3-VL-8B-Instructæ¨¡å‹ (è®¾å¤‡: {device.upper()})...")
    print(f"   æ¨¡å‹è·¯å¾„: {os.path.abspath(model_path)}")

    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_path}")
        print("ğŸ’¡ è¯·å…ˆä¸‹è½½æ¨¡å‹: ./download_model.sh")
        raise FileNotFoundError(f"æ¨¡å‹ç›®å½• {model_path} ä¸å­˜åœ¨")

    print("ğŸ” æ™ºèƒ½éªŒè¯æ¨¡å‹æ–‡ä»¶...")
    model_valid, validation_msg = smart_verify_qwen3_model(model_path)
    if not model_valid:
        print(f"âŒ æ¨¡å‹éªŒè¯å¤±è´¥: {validation_msg}")
        print("ğŸ’¡ è¯·é‡æ–°ä¸‹è½½å®Œæ•´æ¨¡å‹: ./download_model.sh")
        raise ValueError("æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´æˆ–æŸå")
    else:
        print(validation_msg)

    try:
        print("ğŸ”§ åŠ è½½Qwen3VLProcessor...")
        processor = Qwen3VLProcessor.from_pretrained(
            model_path,
            trust_remote_code=False
        )
        print("âœ… ProcessoråŠ è½½æˆåŠŸ!")

        quant_config = None
        if use_4bit and device == "cuda":
            print("âš¡ å¯ç”¨4-bité‡åŒ– (BitsAndBytes)...")
            try:
                quant_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True
                )
            except Exception as e:
                print(f"âš ï¸  4-bité‡åŒ–åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                print("ğŸ”„ å›é€€åˆ°æ ‡å‡†åŠ è½½...")
                use_4bit = False

        model_kwargs = {
            "trust_remote_code": False,
            "device_map": "auto" if device == "cuda" else "cpu",
            "torch_dtype": torch.bfloat16 if device == "cuda" else torch.float32
        }
        if quant_config:
            model_kwargs["quantization_config"] = quant_config

        print("ğŸ§  åŠ è½½Qwen3VLForConditionalGeneration...")
        start_time = time.time()
        model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_path,
            **model_kwargs
        ).eval()
        load_time = time.time() - start_time
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ! (è€—æ—¶: {load_time:.1f}ç§’)")

        if device == "cuda":
            torch.cuda.empty_cache()
        gc.collect()

        return model, processor

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", file=sys.stderr)
        import traceback
        traceback.print_exc()

        print("\nğŸ› ï¸  è¯¦ç»†æ•…éšœæ’é™¤:", file=sys.stderr)
        print("1. æ¨¡å‹ä¸‹è½½: ./download_model.sh", file=sys.stderr)
        print("2. ä¾èµ–è¦æ±‚:", file=sys.stderr)
        print("   â€¢ PyTorch 2.4.1 + CUDA 12.4/12.1/13.0+", file=sys.stderr)
        print("   â€¢ bitsandbytes>=0.44.0", file=sys.stderr)
        print("   â€¢ transformersæºç å®‰è£… (>=4.40.0)", file=sys.stderr)
        print("   â€¢ numpy<2.0", file=sys.stderr)
        sys.exit(1)


def _postprocess_caption(caption: str) -> str:
    """åå¤„ç†ï¼šè¿‡æ»¤ä¸»è§‚è¯ + æ ¼å¼æ ‡å‡†åŒ– + å¼ºåˆ¶ä¸­æ–‡"""
    # ç§»é™¤ç³»ç»Ÿ/ç”¨æˆ·å‰ç¼€
    if "assistant" in caption:
        caption = caption.split("assistant")[-1].strip()

    # ç§»é™¤ä¸»è§‚è¯
    for word in SUBJECTIVE_WORDS:
        caption = caption.replace(word, "")

    # æ ‡å‡†åŒ–æ ‡ç‚¹ (è‹±æ–‡é€—å·åˆ†éš”)
    caption = caption.replace("ï¼Œ", ",").replace("ã€", ",").replace("ã€‚", "").replace("ï¼›", ",")

    # ç§»é™¤å¤šä½™ç©ºæ ¼
    caption = ",".join([part.strip() for part in caption.split(",") if part.strip()])

    # æˆªæ–­è‡³200å­—ç¬¦
    if len(caption) > 200:
        parts = caption.split(",")
        caption = ",".join(parts[:12]) + "..."

    # ç¡®ä¿ä»¥å…³é”®è¯ç»“å°¾
    caption = caption.rstrip(",. ")

    return caption


# âœ… æ ¸å¿ƒä¿®å¤: ä¸¥æ ¼éµå¾ªQwen3-VLå®˜æ–¹API + å¼ºåˆ¶ä¸­æ–‡è¾“å‡º
def generate_chinese_caption(image_path: str, max_new_tokens: int = 160):
    """ä½¿ç”¨Qwen3-VLç”Ÿæˆ100%ä¸­æ–‡è®­ç»ƒä¸“ç”¨caption"""
    global model, processor

    try:
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")

        # æ‰“å¼€å¹¶éªŒè¯å›¾ç‰‡
        image = Image.open(image_path).convert("RGB")
        image.verify()
        image = Image.open(image_path).convert("RGB")

        # âœ… æ ¸å¿ƒ: messagesä¸­ä½¿ç”¨å›¾åƒæ–‡ä»¶è·¯å¾„ï¼ˆå­—ç¬¦ä¸²ï¼‰
        messages = [
            {"role": "system", "content": CAPTION_PROMPT},
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image_path},  # âœ… æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²
                    {"type": "text", "text": "ç”Ÿæˆæ–‡ç”Ÿå›¾æ¨¡å‹è®­ç»ƒç”¨ä¸­æ–‡captionï¼Œç¦ç”¨æ‰€æœ‰è‹±æ–‡æè¿°ï¼Œå¿…é¡»ä½¿ç”¨ä¸­æ–‡è‡ªç„¶è¯­å¥æè¿°"}
                ]
            }
        ]

        # å¤„ç†è¾“å…¥
        text = processor.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # âœ… æ ¸å¿ƒ: processor()ä¸­ä¼ å…¥PIL Imageå¯¹è±¡
        inputs = processor(
            text=[text],
            images=[image],  # âœ… PIL Imageå¯¹è±¡
            return_tensors="pt",
            padding=True
        ).to(model.device)

        # ç”Ÿæˆ (è°ƒæ•´å‚æ•°ä¼˜åŒ–ä¸­æ–‡ç”Ÿæˆ)
        start_time = time.time()
        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.7,  # âœ… æé«˜temperatureå¢å¼ºåˆ›é€ æ€§(ä¸­æ–‡)
                do_sample=True,
                top_p=0.8,
                top_k=20,
                repetition_penalty=1.2
            )
        gen_time = time.time() - start_time

        # è§£ç 
        caption_raw = processor.decode(output[0], skip_special_tokens=True)
        caption_clean = _postprocess_caption(caption_raw)

        print(f"â±ï¸  ç”Ÿæˆè€—æ—¶: {gen_time:.1f}ç§’ | é•¿åº¦: {len(caption_clean)}å­—ç¬¦")
        print(f"   æè¿°: {caption_clean[:80]}...")
        return caption_clean

    except Exception as e:
        print(f"âŒ å¤„ç† {os.path.basename(image_path)} æ—¶å‡ºé”™: {str(e)}")
        if device == "cuda":
            torch.cuda.empty_cache()
        gc.collect()
        return None


def process_images(folder_path: str, use_4bit: bool = False, use_cpu: bool = False, progress=None):
    """æ‰¹é‡å¤„ç†å›¾ç‰‡æ–‡ä»¶å¤¹"""
    if not folder_path or not folder_path.strip():
        return "âŒ é”™è¯¯: è¯·è¾“å…¥æœ‰æ•ˆçš„æ–‡ä»¶å¤¹è·¯å¾„"

    folder_path = folder_path.strip()
    if not os.path.isdir(folder_path):
        return f"âŒ é”™è¯¯: è·¯å¾„ '{folder_path}' ä¸æ˜¯æœ‰æ•ˆæ–‡ä»¶å¤¹"

    SUPPORTED_FORMATS = {'.jpg', '.jpeg', '.png', '.bmp', '.webp', '.tiff'}

    image_files = [
        f for f in os.listdir(folder_path)
        if os.path.splitext(f.lower())[1] in SUPPORTED_FORMATS and
           not f.lower().startswith('._')
    ]

    if not image_files:
        return f"âš ï¸ è­¦å‘Š: åœ¨ '{folder_path}' ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶"

    load_qwen3_model(use_4bit=use_4bit, use_cpu=use_cpu)

    results = {
        "total": len(image_files),
        "success": 0,
        "failed": 0,
        "skipped": 0,
        "details": []
    }

    total = len(image_files)

    for i, filename in enumerate(image_files):
        if progress:
            progress(i / total, desc=f"å¤„ç†ä¸­ ({i + 1}/{total}) - {filename}")

        image_path = os.path.join(folder_path, filename)
        txt_path = os.path.splitext(image_path)[0] + '.txt'

        if os.path.exists(txt_path):
            results["skipped"] += 1
            results["details"].append(f"â­ è·³è¿‡: {filename} (å·²å­˜åœ¨æè¿°æ–‡ä»¶)")
            continue

        print(f"\nğŸ–¼ï¸  å¤„ç†: {filename}")
        caption = generate_chinese_caption(image_path)

        if caption and len(caption) > 30:
            try:
                with open(txt_path, 'w', encoding='utf-8') as f:
                    f.write(caption)
                results["success"] += 1
                preview = caption[:70] + "..." if len(caption) > 70 else caption
                results["details"].append(f"âœ… æˆåŠŸ: {filename}\n   {preview}")
            except Exception as e:
                results["failed"] += 1
                results["details"].append(f"âŒ å†™å…¥å¤±è´¥: {filename}\n   {str(e)}")
        else:
            results["failed"] += 1
            results["details"].append(f"âŒ ç”Ÿæˆå¤±è´¥: {filename}")

        if i % 3 == 0:
            if device == "cuda":
                torch.cuda.empty_cache()
            gc.collect()

    processed = max(1, results["total"] - results["skipped"])
    success_rate = results["success"] / processed * 100

    report = (
            f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!\n\n"
            f"ğŸ“Š æ€»è®¡: {results['total']} å¼ å›¾ç‰‡\n"
            f"âœ… æˆåŠŸ: {results['success']} ({success_rate:.1f}%)\n"
            f"âŒ å¤±è´¥: {results['failed']}\n"
            f"â­ è·³è¿‡: {results['skipped']} (å·²å­˜åœ¨)\n\n"
            f"ğŸ“ ç»“æœä¿å­˜åœ¨: {folder_path}\n\n"
            f"ğŸ“‹ è¯¦ç»†æ—¥å¿— (æœ€è¿‘10æ¡):\n" +
            "\n".join(results["details"][-10:])
    )

    if device == "cuda":
        torch.cuda.empty_cache()
    gc.collect()

    return report


def get_system_info():
    """è·å–ç³»ç»Ÿä¿¡æ¯"""
    try:
        gpu_info = "æœªæ£€æµ‹åˆ°GPU"
        if device == "cuda":
            props = torch.cuda.get_device_properties(0)
            gpu_info = f"{props.name} ({props.total_memory / 1e9:.1f}GB)"

        mem = psutil.virtual_memory()
        disk = psutil.disk_usage(os.path.abspath("."))

        model_status = "âœ… å·²åŠ è½½" if model is not None else "â³ æœªåŠ è½½"
        quant_status = " (4-bit)" if global_use_4bit and model is not None else ""

        model_size = "æœªçŸ¥"
        if model is not None:
            total_params = sum(p.numel() for p in model.parameters())
            model_size = f"{total_params / 1e9:.1f}B"

        transformers_version = "æœªçŸ¥"
        try:
            import transformers
            transformers_version = transformers.__version__
        except:
            pass

        numpy_version = "æœªçŸ¥"
        try:
            import numpy as np
            numpy_version = np.__version__
        except:
            pass

        import huggingface_hub
        patch_status = "âœ… å·²æ³¨å…¥" if hasattr(huggingface_hub, 'HfFolder') else "âŒ æœªæ³¨å…¥"

        return (
            f"**æ“ä½œç³»ç»Ÿ**: {platform.system()} {platform.release()}\n"
            f"**Pythonç‰ˆæœ¬**: {platform.python_version()}\n"
            f"**PyTorchç‰ˆæœ¬**: {torch.__version__} (CUDA: {torch.version.cuda if torch.cuda.is_available() else 'N/A'})\n"
            f"**Transformersç‰ˆæœ¬**: {transformers_version}\n"
            f"**huggingface_hubç‰ˆæœ¬**: {huggingface_hub.__version__} ({patch_status})\n"
            f"**NumPyç‰ˆæœ¬**: {numpy_version}\n"
            f"**è¿è¡Œè®¾å¤‡**: {device.upper()} ({gpu_info})\n"
            f"**ç³»ç»Ÿå†…å­˜**: {mem.total / 1e9:.1f}GB (å¯ç”¨: {mem.available / 1e9:.1f}GB)\n"
            f"**ç£ç›˜ç©ºé—´**: {disk.free / 1e9:.1f}GB å¯ç”¨\n"
            f"**æ¨¡å‹çŠ¶æ€**: {model_status}{quant_status}\n"
            f"**æ¨¡å‹å¤§å°**: Qwen3-VL-8B ({model_size})\n"
            f"**æ¨¡å‹è·¯å¾„**: `{os.path.abspath(model_path)}`"
        )
    except Exception as e:
        return f"âš ï¸ è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {str(e)}"


def create_ui():
    """åˆ›å»ºGradio UIç•Œé¢ (å…¼å®¹Gradio 3.x/4.x)"""
    with gr.Blocks(title="XXGç¦»çº¿å›¾ç‰‡ä¸­æ–‡æ‰“æ ‡å·¥å…· Ver.2.2 (Qwen3-VL)") as demo:
        gr.Markdown("# ğŸ–¼ï¸ Qwen3-VL-8B-Instruct ç¦»çº¿å›¾ç‰‡ä¸­æ–‡æ‰“æ ‡å·¥å…·")
        gr.Markdown("### 100%ä¸­æ–‡captionç”Ÿæˆ Â· éšç§å®‰å…¨ Â· æ–‡ç”Ÿå›¾è®­ç»ƒä¸“ç”¨")
        gr.Markdown("### By è¥¿å°ç“œ / Wechat:priest-mos")

        with gr.Tabs():
            with gr.TabItem("ğŸš€ æ‰¹é‡æ‰“æ ‡"):
                with gr.Row():
                    with gr.Column(scale=3):
                        folder_input = gr.Textbox(
                            label="ğŸ“ å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„",
                            placeholder="ä¾‹å¦‚: C:/images æˆ– /home/user/photos",
                            value=os.path.join(os.path.expanduser("~"), "ai-toolkit/datasets/demo")
                        )
                        with gr.Row():
                            process_btn = gr.Button("ğŸš€ å¼€å§‹ç”Ÿæˆä¸­æ–‡caption", variant="primary")
                            stop_btn = gr.Button("ğŸ›‘ åœæ­¢", variant="stop")

                        with gr.Row():
                            use_4bit = gr.Checkbox(
                                label="å¯ç”¨4-bité‡åŒ– (ä½æ˜¾å­˜æ¨¡å¼)",
                                value=False,
                                info="é€‚ç”¨äº6GBä»¥ä¸‹æ˜¾å­˜çš„GPU"
                            )
                            use_cpu = gr.Checkbox(
                                label="å¼ºåˆ¶CPUæ¨¡å¼",
                                value=False,
                                info="æ— GPUæ—¶ä½¿ç”¨"
                            )

                        output = gr.Textbox(label="ğŸ“ å¤„ç†ç»“æœ", lines=15, interactive=False)

                    with gr.Column(scale=2):
                        sys_info = gr.Markdown(label="ğŸ”§ ç³»ç»Ÿä¿¡æ¯")
                        refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°ç³»ç»Ÿä¿¡æ¯", size="sm")
                        refresh_btn.click(fn=get_system_info, outputs=sys_info)

        demo.load(fn=get_system_info, outputs=sys_info)

        process_btn.click(
            fn=process_images,
            inputs=[folder_input, use_4bit, use_cpu],
            outputs=output,
            show_progress="full"
        )

        stop_btn.click(
            fn=lambda: "â¹ï¸ æ“ä½œå·²åœæ­¢",
            outputs=output
        )

        gr.Markdown("### ğŸ“ ä½¿ç”¨æŒ‡å—")
        gr.Markdown("""
        #### **æ ¸å¿ƒä¼˜åŠ¿**
        - âœ… **100%ä¸­æ–‡caption**ï¼šéå…³é”®è¯å †ç Œï¼Œè¾“å‡ºä¸­æ–‡è‡ªç„¶è¯­å¥æè¿°
        - âœ… **ä¸­æ–‡æç¤ºè¯æ–‡ç”Ÿå›¾æ¨¡å‹è®­ç»ƒä¸“ç”¨**ï¼š`ä¸€ä½å¹´è½»ä¸œäºšå¥³æ€§,æŠ«æ•£ç€ä¹Œé»‘æ³¢æµªé•¿å‘,ä½©æˆ´å®½æªç¼–ç»‡è‰å¸½...`
        - âœ… **100%ç¦»çº¿è¿è¡Œ**ï¼šæ— ä»»ä½•ç½‘ç»œè¯·æ±‚

        #### **è¾“å‡ºç¤ºä¾‹**
        ```
        ä¸€ä½å¹´è½»ä¸œäºšå¥³æ€§,æŠ«æ•£ç€ä¹Œé»‘æ³¢æµªé•¿å‘,ä½©æˆ´å®½æªç¼–ç»‡è‰å¸½,èº«ç©¿å°æœ‰å½©è‰²é¸Ÿç±»å›¾æ¡ˆçš„ç™½è‰²åŠå¸¦è¿è¡£è£™,èº«ä½“å¾®å¾®ä¾§å‘é•œå¤´,å³æ‰‹è‡ªç„¶å‚è½æ¡æŒç‰©ä½“,ç«™åœ¨æµ…ç°è‰²å¢™é¢æ—,å¤´é¡¶ä¸Šæ–¹æ”€çˆ¬ç”Ÿé•¿ç€èŒ‚å¯†ç¿ ç»¿æ¤ç‰©,é˜³å…‰è‡ªå‰æ–¹æŸ”å’Œæ´’è½ç…§äº®äººç‰©æ­£é¢,æ•´å¹…ç”»é¢å‘ˆç°æ¸…æ–°æ˜äº®çš„æ—¥ç³»æˆ·å¤–æ‘„å½±é£æ ¼
        ```
        > âœ… å…¨ä¸­æ–‡æè¿° | âœ… ä¼˜åŒ–å¤šç§åœºæ™¯ | âœ… åŒ…å«æ„å›¾/é£æ ¼æ ‡ç­¾

        #### **æ“ä½œæ­¥éª¤**
        1. å¡«å†™å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
        2. ä½æ˜¾å­˜GPUï¼šå‹¾é€‰"å¯ç”¨4-bité‡åŒ–"
        3. ç‚¹å‡»"ğŸš€ å¼€å§‹ç”Ÿæˆä¸­æ–‡caption"
        """)

        gr.Markdown(
            "<div style='text-align: center; margin-top: 20px; color: #888;'>"
            "Â© 2026 XXGç¦»çº¿å›¾ç‰‡ä¸­æ–‡æ‰“æ ‡å·¥å…·ï¼ˆQwen3-VLï¼‰ | 100%ä¸­æ–‡caption Â· éšç§å®‰å…¨<br>"
            "æ ¸å¿ƒæŠ€æœ¯: å¼ºåŒ–ä¸­æ–‡prompt + ä¸­æ–‡è´¨é‡éªŒè¯ + ç¦»çº¿æ¨¡å‹"
            "</div>"
        )

    return demo


def main():
    """ä¸»å‡½æ•°"""
    global global_use_4bit

    parser = argparse.ArgumentParser(description='Qwen3-VLç¦»çº¿å›¾ç‰‡ä¸­æ–‡æ‰“æ ‡å·¥å…·')
    parser.add_argument('--4bit', action='store_true', help='å¯ç”¨4-bité‡åŒ–')
    parser.add_argument('--cpu', action='store_true', help='å¼ºåˆ¶CPUæ¨¡å¼')
    parser.add_argument('--port', type=int, default=9527, help='Web UIç«¯å£')
    parser.add_argument('--folder', type=str, help='ç›´æ¥å¤„ç†æ–‡ä»¶å¤¹')
    args = parser.parse_args()

    global_use_4bit = args.__dict__['4bit']

    if args.__dict__['4bit']:
        print("âš¡ å¯åŠ¨4-bité‡åŒ–æ¨¡å¼")
    if args.cpu:
        global device
        device = "cpu"
        print("ğŸ’» å¼ºåˆ¶CPUæ¨¡å¼")

    print("=" * 70)
    print("ğŸ–¼ï¸  XXGç¦»çº¿å›¾ç‰‡ä¸­æ–‡æ‰“æ ‡å·¥å…· Ver.2.2 (Qwen3-VL)")
    print("âœ… 100%ä¸­æ–‡captionç”Ÿæˆ | âœ… æœ¬åœ°æ¨¡å‹åŒ–")
    print("=" * 70)

    check_system_resources()

    if args.folder:
        print(f"\nğŸ“ ç›´æ¥å¤„ç†æ–‡ä»¶å¤¹: {args.folder}")
        result = process_images(args.folder, use_4bit=args.__dict__['4bit'], use_cpu=args.cpu)
        print("\n" + result)
        return

    demo = create_ui()
    demo.launch(
        server_name="127.0.0.1",
        server_port=args.port,
        share=False,
        show_error=True,
        quiet=True,
        theme=gr.themes.Soft()
    )


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºå·²å®‰å…¨é€€å‡º")
    except Exception as e:
        print(f"âŒ ä¸¥é‡é”™è¯¯: {str(e)}", file=sys.stderr)
        import traceback

        traceback.print_exc()
        sys.exit(1)