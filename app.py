#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šä¹‰åƒé—®ç¦»çº¿å›¾ç‰‡ä¸­æ–‡æ‰“æ ‡å·¥å…·
- 100%ç¦»çº¿è¿è¡Œ
- éšç§å®‰å…¨ä¿æŠ¤
- ä¸“ä¸šçº§ä¸­æ–‡æè¿°
- é€‚é…Qwen-VL-Chatå®é™…æ–‡ä»¶ç»“æ„
- ä¿®å¤æ‰€æœ‰ä¾èµ–å’Œå…¼å®¹æ€§é—®é¢˜
"""

import os
import sys
import time
import gc
import json
import argparse
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡ (å¿…é¡»åœ¨å¯¼å…¥torchå‰è®¾ç½®)
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"  # åŠ é€Ÿä¸‹è½½
os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "300"  # 5åˆ†é’Ÿè¶…æ—¶
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # é¿å…è­¦å‘Š
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"

# ä¿®å¤NumPyå…¼å®¹æ€§é—®é¢˜
try:
    import numpy as np

    if np.__version__.startswith("2"):
        print(f"âš ï¸  æ£€æµ‹åˆ°NumPy {np.__version__}ï¼Œå¯èƒ½ä¸PyTorchä¸å…¼å®¹")
        print("ğŸ’¡ å»ºè®®è¿è¡Œ: pip install numpy==1.26.4 --upgrade")
except ImportError:
    pass

# é¦–å…ˆå¯¼å…¥torch
try:
    import torch
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥torch: {str(e)}")
    print("ğŸ’¡ è¯·å…ˆå®‰è£…ä¾èµ–: pip install -r requirements.txt")
    sys.exit(1)

# å…¨å±€å˜é‡ (ç°åœ¨å¯ä»¥å®‰å…¨ä½¿ç”¨torch)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = None
tokenizer = None
model_path = "./qwen_models"
global_use_4bit = False  # é‡å‘½åå…¨å±€å˜é‡ï¼Œé¿å…ä¸å‡½æ•°å‚æ•°å†²çª

# å¯¼å…¥å…¶ä»–ä¾èµ–
try:
    from PIL import Image
    import gradio as gr
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from tqdm import tqdm
    import psutil
    import platform
except ImportError as e:
    print(f"âŒ ä¾èµ–å¯¼å…¥å¤±è´¥: {str(e)}")
    print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
    print("1. å®‰è£…ä¾èµ–: pip install -r requirements.txt")
    print("2. ç¡®ä¿è™šæ‹Ÿç¯å¢ƒå·²æ¿€æ´»")
    sys.exit(1)


def check_system_resources():
    """æ£€æŸ¥ç³»ç»Ÿèµ„æºæ˜¯å¦æ»¡è¶³è¦æ±‚"""
    print("ğŸ” ç³»ç»Ÿèµ„æºæ£€æŸ¥...")

    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    disk = psutil.disk_usage(os.path.abspath("."))
    free_gb = disk.free / (1024 ** 3)
    print(f"ğŸ’¾ ç£ç›˜ç©ºé—´: {free_gb:.1f}GB å¯ç”¨")
    if free_gb < 8:
        print(f"âš ï¸  è­¦å‘Š: ç£ç›˜ç©ºé—´ä¸è¶³! å»ºè®®è‡³å°‘8GBç©ºé—²ç©ºé—´")

    # æ£€æŸ¥å†…å­˜
    mem = psutil.virtual_memory()
    available_gb = mem.available / (1024 ** 3)
    total_gb = mem.total / (1024 ** 3)
    print(f"ğŸ§  ç³»ç»Ÿå†…å­˜: {available_gb:.1f}GB/{total_gb:.1f}GB å¯ç”¨")
    if available_gb < 4:
        print(f"âš ï¸  è­¦å‘Š: å¯ç”¨å†…å­˜ä¸è¶³4GBï¼Œå¤„ç†å¤§å›¾æ—¶å¯èƒ½å¤±è´¥")

    # æ£€æŸ¥GPU
    if device == "cuda":
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
        print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)} ({gpu_mem:.1f}GB æ˜¾å­˜)")
        if gpu_mem < 6:
            print(f"âš ï¸  è­¦å‘Š: GPUæ˜¾å­˜å°äº6GBï¼Œå»ºè®®å¯ç”¨4-bité‡åŒ–")
    else:
        print("ğŸ’» ä½¿ç”¨CPUæ¨¡å¼ (æ— GPUåŠ é€Ÿ)")

    return {
        "disk_free_gb": free_gb,
        "mem_available_gb": available_gb,
        "gpu_available": device == "cuda",
        "gpu_mem_gb": torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if device == "cuda" else 0
    }


def smart_verify_qwen_model(model_path):
    """æ™ºèƒ½éªŒè¯Qwen-VL-Chatæ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´"""
    model_dir = Path(model_path)

    if not model_dir.exists() or not model_dir.is_dir():
        return False, "æ¨¡å‹ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•"

    # 1. æ£€æŸ¥åŸºç¡€é…ç½®æ–‡ä»¶
    required_configs = ["config.json", "tokenizer_config.json"]
    missing_configs = [f for f in required_configs if not (model_dir / f).exists()]

    if missing_configs:
        return False, f"ç¼ºå¤±åŸºç¡€é…ç½®æ–‡ä»¶: {missing_configs}"

    # 2. æ£€æŸ¥æƒé‡æ–‡ä»¶ (æ”¯æŒåˆ†ç‰‡)
    weight_files = list(model_dir.glob("pytorch_model*.bin")) + list(model_dir.glob("model*.safetensors"))

    if not weight_files:
        # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶
        if (model_dir / "pytorch_model.bin.index.json").exists():
            return False, "æ£€æµ‹åˆ°ç´¢å¼•æ–‡ä»¶ï¼Œä½†æƒé‡æ–‡ä»¶æœªå®Œå…¨ä¸‹è½½ï¼Œè¯·é‡æ–°ä¸‹è½½"
        return False, "æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶ (pytorch_model*.bin æˆ– model*.safetensors)"

    # 3. æ£€æŸ¥Qwenç‰¹å®štokenizeræ–‡ä»¶
    qwen_tokenizer_files = ["qwen.tiktoken", "tokenization_qwen.py"]
    has_qwen_tokenizer = all((model_dir / f).exists() for f in qwen_tokenizer_files)

    if not has_qwen_tokenizer:
        # æ£€æŸ¥æ ‡å‡†tokenizeræ–‡ä»¶
        std_tokenizer_files = ["special_tokens_map.json", "tokenizer.json"]
        has_std_tokenizer = any((model_dir / f).exists() for f in std_tokenizer_files)

        if not has_std_tokenizer:
            return False, "æœªæ‰¾åˆ°æœ‰æ•ˆçš„tokenizeræ–‡ä»¶ (ç¼ºå°‘Qwenç‰¹å®šæ–‡ä»¶æˆ–æ ‡å‡†tokenizeræ–‡ä»¶)"

    # 4. æ£€æŸ¥æ–‡ä»¶å¤§å°
    total_size = sum(f.stat().st_size for f in weight_files)
    if total_size < 10e9:  # 10GB
        return False, f"æ¨¡å‹æ–‡ä»¶æ€»å¤§å°è¿‡å° ({total_size / 1e9:.2f}GB)ï¼Œå¯èƒ½ä¸‹è½½ä¸å®Œæ•´ (Qwen-VL-Chatçº¦18GB)"

    # 5. éªŒè¯åˆ†ç‰‡æ•°é‡
    shard_files = [f for f in weight_files if "pytorch_model-" in f.name]
    if shard_files:
        # æ£€æŸ¥åˆ†ç‰‡æ•°é‡æ˜¯å¦åˆç† (Qwen-VL-Chatåº”æœ‰10ä¸ªåˆ†ç‰‡)
        if len(shard_files) < 8:  # å…è®¸ç¼ºå¤±1-2ä¸ª
            return False, f"ä»…æ‰¾åˆ° {len(shard_files)} ä¸ªæƒé‡åˆ†ç‰‡ï¼Œæ¨¡å‹å¯èƒ½ä¸å®Œæ•´ (åº”æœ‰10ä¸ªåˆ†ç‰‡)"

    return True, f"âœ… Qwen-VL-Chatæ¨¡å‹éªŒè¯æˆåŠŸ!\n   â€¢ æ‰¾åˆ° {len(weight_files)} ä¸ªæƒé‡æ–‡ä»¶\n   â€¢ æ€»å¤§å°: {total_size / 1e9:.2f}GB\n   â€¢ {'æ£€æµ‹åˆ°Qwenç‰¹å®štokenizer' if has_qwen_tokenizer else 'æ£€æµ‹åˆ°æ ‡å‡†tokenizer'}"


def load_qwen_model(use_4bit=False, use_cpu=False):
    """åŠ è½½Qwen-VL-Chatæ¨¡å‹ï¼Œé€‚é…å®é™…æ–‡ä»¶ç»“æ„"""
    global model, tokenizer, device, global_use_4bit  # ä½¿ç”¨é‡å‘½åçš„å…¨å±€å˜é‡

    # æ›´æ–°å…¨å±€4-bitæ ‡å¿—
    global_use_4bit = use_4bit

    # å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼
    if use_cpu:
        device = "cpu"
        print("âš ï¸  å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼ (æ— GPUåŠ é€Ÿ)")

    if model is not None and tokenizer is not None:
        print("âœ… æ¨¡å‹å·²åœ¨å†…å­˜ä¸­ï¼Œè·³è¿‡åŠ è½½")
        return model, tokenizer

    print(f"ğŸš€ æ­£åœ¨åŠ è½½Qwen-VL-Chatæ¨¡å‹ (è®¾å¤‡: {device.upper()})...")
    print(f"   æ¨¡å‹è·¯å¾„: {os.path.abspath(model_path)}")

    # æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_path}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python download_models.py")
        raise FileNotFoundError(f"æ¨¡å‹ç›®å½• {model_path} ä¸å­˜åœ¨")

    # æ™ºèƒ½éªŒè¯æ¨¡å‹æ–‡ä»¶
    print("ğŸ” æ™ºèƒ½éªŒè¯æ¨¡å‹æ–‡ä»¶...")
    model_valid, validation_msg = smart_verify_qwen_model(model_path)

    if not model_valid:
        print(f"âŒ æ¨¡å‹éªŒè¯å¤±è´¥: {validation_msg}")
        print("ğŸ’¡ è¯·é‡æ–°ä¸‹è½½å®Œæ•´æ¨¡å‹: python download_models.py")
        raise ValueError("æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´æˆ–æŸå")
    else:
        print(validation_msg)

    try:
        # 1ï¸âƒ£ åŠ è½½tokenizer - å…¼å®¹Qwençš„ç‰¹æ®Štokenizer
        print("ğŸ”§ åŠ è½½tokenizer (Qwenç‰¹å®šé…ç½®)...")

        # ç¡®ä¿æ¨¡å‹è·¯å¾„åœ¨Pythonè·¯å¾„ä¸­
        if model_path not in sys.path:
            sys.path.insert(0, model_path)

        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side='left',
            use_fast=False  # Qwenæœ‰æ—¶åœ¨fastæ¨¡å¼ä¸‹æœ‰é—®é¢˜
        )
        print("âœ… TokenizeråŠ è½½æˆåŠŸ! (Qwenç‰¹å®šé…ç½®)")

        # 2ï¸âƒ£ å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°
        model_kwargs = {
            "trust_remote_code": True,
            "cache_dir": model_path,
            "device_map": "auto" if device == "cuda" else "cpu"
        }

        # 3ï¸âƒ£ 4-bité‡åŒ–æ”¯æŒ
        if use_4bit and device == "cuda":
            print("âš¡ å¯ç”¨4-bité‡åŒ– (å‡å°‘æ˜¾å­˜éœ€æ±‚)...")
            try:
                from auto_gptq import AutoGPTQForCausalLM
                model = AutoGPTQForCausalLM.from_quantized(
                    model_path,
                    device="cuda:0",
                    use_triton=False,
                    quantize_config=None
                )
                print("âœ… 4-bité‡åŒ–æ¨¡å‹åŠ è½½æˆåŠŸ!")
            except Exception as e:
                print(f"âš ï¸  4-bitåŠ è½½å¤±è´¥: {str(e)}")
                print("ğŸ”„ å›é€€åˆ°æ ‡å‡†åŠ è½½...")
                use_4bit = False

        # 4ï¸âƒ£ æ ‡å‡†åŠ è½½
        if not use_4bit or model is None:
            print("ğŸ§  åŠ è½½æ ‡å‡†ç²¾åº¦æ¨¡å‹...")
            start_time = time.time()
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                **model_kwargs
            ).eval()
            load_time = time.time() - start_time
            print(f"âœ… æ ‡å‡†ç²¾åº¦æ¨¡å‹åŠ è½½æˆåŠŸ! (è€—æ—¶: {load_time:.1f}ç§’)")

        # 5ï¸âƒ£ éªŒè¯æ¨¡å‹
        print("ğŸ” éªŒè¯æ¨¡å‹åŠŸèƒ½...")
        try:
            test_query = tokenizer.from_list_format([
                {'text': 'ä½ å¥½ï¼Œé€šä¹‰åƒé—®!'}
            ])
            _ = model.chat(tokenizer, query=test_query, history=None, max_new_tokens=10)
            print("âœ… æ¨¡å‹åŠŸèƒ½éªŒè¯é€šè¿‡!")
        except Exception as e:
            print(f"âš ï¸  æ¨¡å‹åŠŸèƒ½éªŒè¯å¤±è´¥ï¼Œä½†å¯èƒ½ä¸å½±å“å›¾ç‰‡æ‰“æ ‡: {str(e)}")

        # 6ï¸âƒ£ èµ„æºä¼˜åŒ–
        if device == "cuda":
            torch.cuda.empty_cache()
        gc.collect()

        return model, tokenizer

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

        # è¯¦ç»†çš„é”™è¯¯å¤„ç† - å¢å¼ºç‰ˆ
        print("\nğŸ› ï¸  è¯¦ç»†æ•…éšœæ’é™¤:")
        print("1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§:")
        print("   â€¢ ç¡®ä¿æœ‰10ä¸ªpytorch_model-XXXXX-of-00010.binæ–‡ä»¶")
        print("   â€¢ ç¡®ä¿æœ‰qwen.tiktokenå’Œtokenization_qwen.py")
        print("   â€¢ ç¡®ä¿æœ‰modeling_qwen.pyå’Œconfiguration_qwen.py")
        print("2. å¿…éœ€ä¾èµ–æ£€æŸ¥:")

        # æ£€æŸ¥transformers_stream_generator
        try:
            import transformers_stream_generator
            print("âœ… transformers_stream_generator (å¿…éœ€ä¾èµ–å·²å®‰è£…)")
        except ImportError as e:
            print("âŒ transformers_stream_generator æœªå®‰è£…! (å…³é”®ä¾èµ–)")
            print("ğŸ’¡ ä¿®å¤å‘½ä»¤: pip install transformers_stream_generator==0.0.4 --upgrade")

        # æ£€æŸ¥tiktoken (å…¼å®¹0.7.0+ç‰ˆæœ¬)
        try:
            import tiktoken
            # å®‰å…¨æ£€æŸ¥ç‰ˆæœ¬å±æ€§
            version = getattr(tiktoken, '__version__', '0.7.0+ (æ— __version__å±æ€§)')
            print(f"âœ… tiktoken ç‰ˆæœ¬: {version} (å·²å®‰è£…)")
        except ImportError as e:
            print("âŒ tiktoken æœªå®‰è£…! (å…³é”®ä¾èµ–)")
            print("ğŸ’¡ ä¿®å¤å‘½ä»¤: pip install tiktoken==0.6.0 --upgrade")

        # æ£€æŸ¥Qwen-VLç‰¹å®šå·¥å…·
        try:
            import qwen_vl_utils
            print("âœ… qwen_vl_utils (Qwen-VLä¸“ç”¨å·¥å…·å·²å®‰è£…)")
        except ImportError as e:
            print("âš ï¸  qwen_vl_utils æœªå®‰è£… (éå¿…éœ€ï¼Œä½†æ¨è)")
            print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install qwen_vl_utils==0.0.1 --upgrade")

        # æ£€æŸ¥auto-gptq (4-bité‡åŒ–)
        if use_4bit:
            try:
                import auto_gptq
                print("âœ… auto_gptq (4-bité‡åŒ–æ”¯æŒå·²å®‰è£…)")
            except ImportError as e:
                print("âš ï¸  auto_gptq æœªå®‰è£… (4-bité‡åŒ–éœ€è¦)")
                print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install auto-gptq==0.7.1 --upgrade")

        print("\n3. ä¾èµ–ç‰ˆæœ¬è¦æ±‚:")
        print("   â€¢ transformers==4.37.0")
        print("   â€¢ torch==2.2.0")
        print("   â€¢ numpy==1.26.4")
        print("   â€¢ è¯·è¿è¡Œ: pip install -r requirements.txt --upgrade")

        print("\n4. å°è¯•é‡æ–°ä¸‹è½½å’Œä¿®å¤:")
        print("   python fix_dependencies.py --full")
        print("   python download_models.py")

        # æä¾›ä¸€é”®ä¿®å¤å‘½ä»¤
        print("\nğŸ”§ ä¸€é”®ä¿®å¤æ‰€æœ‰ä¾èµ– (æ¨è):")
        print("   pip uninstall -y transformers_stream_generator tiktoken auto-gptq")
        print("   pip install -r requirements.txt --force-reinstall --no-cache-dir")

        sys.exit(1)


def generate_chinese_caption(image_path, max_new_tokens=200):
    """ä½¿ç”¨Qwen-VL-Chatç”Ÿæˆä¸­æ–‡å›¾ç‰‡æè¿°"""
    global model, tokenizer

    try:
        # éªŒè¯å›¾ç‰‡
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")

        # æ‰“å¼€å›¾ç‰‡
        image = Image.open(image_path).convert("RGB")
        image.verify()  # éªŒè¯å›¾ç‰‡å®Œæ•´æ€§

        # å‡†å¤‡æŸ¥è¯¢
        query = tokenizer.from_list_format([
            {'image': image_path},
            {
                'text': 'è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼Œç”¨ä¸­æ–‡å›ç­”ã€‚éœ€è¦åŒ…å«ï¼šä¸»è¦ç‰©ä½“ã€åœºæ™¯ç¯å¢ƒã€é¢œè‰²ç‰¹å¾ã€æ–‡å­—å†…å®¹åŠå­—ä½“ã€å›¾æ–‡æ’ç‰ˆå¸ƒå±€ã€äººä½“ç»“æ„å’Œæ¯”ä¾‹ï¼ˆå¦‚æœæœ‰ï¼‰ã€äººç‰©åŠ¨ä½œï¼ˆå¦‚æœæœ‰ï¼‰ã€æ•´ä½“æ°›å›´ç­‰å…³é”®ä¿¡æ¯ã€‚è¦æ±‚æè¿°ä¸“ä¸šã€å‡†ç¡®ã€æµç•…ã€‚'}
        ])

        # ç”Ÿæˆæè¿°
        start_time = time.time()
        with torch.no_grad():
            response, _ = model.chat(
                tokenizer,
                query=query,
                history=None,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                top_p=0.9
            )
        gen_time = time.time() - start_time

        # åå¤„ç†
        caption = response.strip()
        caption = caption.replace('\n', ' ').replace('  ', ' ')

        print(f"â±ï¸  ç”Ÿæˆè€—æ—¶: {gen_time:.1f}ç§’, æè¿°é•¿åº¦: {len(caption)}å­—ç¬¦")
        return caption

    except Exception as e:
        print(f"âŒ å¤„ç† {os.path.basename(image_path)} æ—¶å‡ºé”™: {str(e)}")
        # æ¸…ç†èµ„æº
        if device == "cuda":
            torch.cuda.empty_cache()
        gc.collect()
        return None


def process_images(folder_path, use_4bit=False, use_cpu=False, progress=gr.Progress()):
    """æ‰¹é‡å¤„ç†å›¾ç‰‡æ–‡ä»¶å¤¹ï¼Œç”Ÿæˆä¸­æ–‡æè¿°"""
    if not folder_path or not folder_path.strip():
        return "âŒ é”™è¯¯: è¯·è¾“å…¥æœ‰æ•ˆçš„æ–‡ä»¶å¤¹è·¯å¾„"

    folder_path = folder_path.strip()
    if not os.path.isdir(folder_path):
        return f"âŒ é”™è¯¯: è·¯å¾„ '{folder_path}' ä¸æ˜¯æœ‰æ•ˆæ–‡ä»¶å¤¹"

    # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
    SUPPORTED_FORMATS = {'.jpg', '.jpeg', '.png', '.bmp', '.webp', '.tiff'}

    # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
    image_files = [
        f for f in os.listdir(folder_path)
        if os.path.splitext(f.lower())[1] in SUPPORTED_FORMATS and
           not f.lower().startswith('._')  # è·³è¿‡macOSä¸´æ—¶æ–‡ä»¶
    ]

    if not image_files:
        return f"âš ï¸ è­¦å‘Š: åœ¨ '{folder_path}' ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶\næ”¯æŒæ ¼å¼: {', '.join(SUPPORTED_FORMATS)}"

    # åŠ è½½æ¨¡å‹
    load_qwen_model(use_4bit=use_4bit, use_cpu=use_cpu)

    # å‡†å¤‡ç»“æœ
    results = {
        "total": len(image_files),
        "success": 0,
        "failed": 0,
        "skipped": 0,
        "details": []
    }

    total = len(image_files)

    # å¤„ç†æ¯å¼ å›¾ç‰‡
    for i, filename in enumerate(image_files):
        progress(i / total, desc=f"å¤„ç†ä¸­ ({i + 1}/{total}) - {filename}")

        image_path = os.path.join(folder_path, filename)
        txt_path = os.path.splitext(image_path)[0] + '.txt'

        # è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶
        if os.path.exists(txt_path):
            results["skipped"] += 1
            results["details"].append(f"â­ è·³è¿‡: {filename} (å·²å­˜åœ¨æè¿°æ–‡ä»¶)")
            continue

        # ç”Ÿæˆæè¿°
        print(f"\nğŸ–¼ï¸  å¤„ç†: {filename}")
        caption = generate_chinese_caption(image_path)

        # ä¿å­˜ç»“æœ
        if caption and len(caption) > 20:  # ç¡®ä¿æè¿°æœ‰æ„ä¹‰
            try:
                with open(txt_path, 'w', encoding='utf-8') as f:
                    f.write(caption)
                results["success"] += 1
                preview = caption[:70] + "..." if len(caption) > 70 else caption
                results["details"].append(f"âœ… æˆåŠŸ: {filename}\n   {preview}")
                print(f"   æè¿°: {preview}")
            except Exception as e:
                results["failed"] += 1
                results["details"].append(f"âŒ å†™å…¥å¤±è´¥: {filename}\n   {str(e)}")
        else:
            results["failed"] += 1
            results["details"].append(f"âŒ ç”Ÿæˆå¤±è´¥: {filename}" + (f"\n   åŸå› : {caption}" if caption else ""))

        # èµ„æºæ¸…ç†
        if i % 3 == 0:
            if device == "cuda":
                torch.cuda.empty_cache()
            gc.collect()

    # ç”ŸæˆæŠ¥å‘Š
    processed = max(1, results["total"] - results["skipped"])
    success_rate = results["success"] / processed * 100

    report = (
            f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!\n\n"
            f"ğŸ“Š æ€»è®¡: {results['total']} å¼ å›¾ç‰‡\n"
            f"âœ… æˆåŠŸ: {results['success']} ({success_rate:.1f}%)\n"
            f"âŒ å¤±è´¥: {results['failed']}\n"
            f"â­ è·³è¿‡: {results['skipped']} (å·²å­˜åœ¨)\n\n"
            f"ğŸ“ ç»“æœä¿å­˜åœ¨: {folder_path}\n\n"
            f"ğŸ“‹ è¯¦ç»†æ—¥å¿— (æœ€è¿‘10æ¡):\n" +
            "\n".join(results["details"][-10:])
    )

    # æœ€ç»ˆèµ„æºæ¸…ç†
    if device == "cuda":
        torch.cuda.empty_cache()
    gc.collect()

    return report


def get_system_info():
    """è·å–ç³»ç»Ÿä¿¡æ¯ç”¨äºUIæ˜¾ç¤º"""
    try:
        gpu_info = "æœªæ£€æµ‹åˆ°GPU"
        if device == "cuda":
            gpu_info = f"{torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB)"

        mem = psutil.virtual_memory()
        disk = psutil.disk_usage(os.path.abspath("."))

        model_status = "âœ… å·²åŠ è½½" if model is not None else "â³ æœªåŠ è½½"
        quant_status = " (4-bit)" if global_use_4bit and model is not None else ""

        tokenizer_type = "æœªçŸ¥"
        if tokenizer is not None:
            if hasattr(tokenizer, 'name_or_path') and 'qwen' in tokenizer.name_or_path.lower():
                tokenizer_type = "Qwenç‰¹å®štokenizer"
            else:
                tokenizer_type = "æ ‡å‡†tokenizer"

        model_size = "æœªçŸ¥"
        if model is not None:
            total_params = sum(p.numel() for p in model.parameters())
            model_size = f"{total_params / 1e9:.1f}B"

        numpy_version = "æœªçŸ¥"
        try:
            import numpy as np
            numpy_version = np.__version__
        except:
            pass

        return (
            f"**æ“ä½œç³»ç»Ÿ**: {platform.system()} {platform.release()}\n"
            f"**Pythonç‰ˆæœ¬**: {platform.python_version()}\n"
            f"**NumPyç‰ˆæœ¬**: {numpy_version} (éœ€è¦1.26.4)\n"
            f"**è¿è¡Œè®¾å¤‡**: {device.upper()} ({gpu_info})\n"
            f"**ç³»ç»Ÿå†…å­˜**: {mem.total / 1e9:.1f}GB (å¯ç”¨: {mem.available / 1e9:.1f}GB)\n"
            f"**ç£ç›˜ç©ºé—´**: {disk.free / 1e9:.1f}GB å¯ç”¨\n"
            f"**æ¨¡å‹çŠ¶æ€**: {model_status}{quant_status}\n"
            f"**æ¨¡å‹å¤§å°**: {model_size}\n"
            f"**Tokenizerç±»å‹**: {tokenizer_type}\n"
            f"**æ¨¡å‹è·¯å¾„**: `{os.path.abspath(model_path)}`"
        )
    except Exception as e:
        return f"âš ï¸ è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {str(e)}"


def create_ui():
    """åˆ›å»ºGradio UIç•Œé¢"""
    with gr.Blocks(title="é€šä¹‰åƒé—®ç¦»çº¿å›¾ç‰‡æ‰“æ ‡å·¥å…·", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ–¼ï¸ é€šä¹‰åƒé—®ç¦»çº¿å›¾ç‰‡ä¸­æ–‡æ‰“æ ‡å·¥å…·")
        gr.Markdown("### 100%ç¦»çº¿è¿è¡Œ Â· éšç§å®‰å…¨ Â· ä¸“ä¸šçº§ä¸­æ–‡æè¿°")

        with gr.Tabs():
            with gr.TabItem("ğŸš€ å¤„ç†å›¾ç‰‡"):
                with gr.Row():
                    with gr.Column(scale=3):
                        folder_input = gr.Textbox(
                            label="ğŸ“ å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„",
                            placeholder="ä¾‹å¦‚: C:/images æˆ– /home/user/photos",
                            value=os.path.join(os.path.expanduser("~"), "Pictures")
                        )
                        with gr.Row():
                            process_btn = gr.Button("ğŸš€ å¼€å§‹ä¸­æ–‡æ‰“æ ‡", variant="primary")
                            stop_btn = gr.Button("ğŸ›‘ åœæ­¢", variant="stop")

                        with gr.Row():
                            use_4bit = gr.Checkbox(
                                label="å¯ç”¨4-bité‡åŒ– (ä½æ˜¾å­˜æ¨¡å¼)",
                                value=False,
                                info="é€‚ç”¨äº6GBä»¥ä¸‹æ˜¾å­˜çš„GPUï¼Œå¤„ç†é€Ÿåº¦ç¨æ…¢ä½†å†…å­˜éœ€æ±‚å¤§å¹…é™ä½"
                            )
                            use_cpu = gr.Checkbox(
                                label="å¼ºåˆ¶CPUæ¨¡å¼",
                                value=False,
                                info="æ— GPUæˆ–GPUä¸ç¨³å®šæ—¶ä½¿ç”¨ï¼Œé€Ÿåº¦è¾ƒæ…¢ä½†æ›´ç¨³å®š"
                            )

                        output = gr.Textbox(label="ğŸ“ å¤„ç†ç»“æœ", lines=15, interactive=False)

                    with gr.Column(scale=2):
                        sys_info = gr.Markdown(label="ğŸ”§ ç³»ç»Ÿä¿¡æ¯")
                        demo.load(get_system_info, None, sys_info, every=30)

        # äº‹ä»¶å¤„ç†
        process_btn.click(
            fn=process_images,
            inputs=[folder_input, use_4bit, use_cpu],
            outputs=output,
            show_progress="full"
        )

        stop_btn.click(
            fn=lambda: "â¹ï¸ æ“ä½œå·²åœæ­¢ (å¯èƒ½éœ€è¦ç­‰å¾…å½“å‰å›¾ç‰‡å¤„ç†å®Œæˆ)",
            outputs=output
        )

        gr.Markdown("### ğŸ“ ä½¿ç”¨æŒ‡å—")
        gr.Markdown("""
        #### **é‡è¦æç¤º: ä¾èµ–å…¼å®¹æ€§**
        - **NumPyå¿…é¡»ä¸º1.26.4ç‰ˆæœ¬** (PyTorch 2.2.0ä¸å…¼å®¹NumPy 2.x)
        - **tiktokenå¿…é¡»ä¸º0.6.0ç‰ˆæœ¬** (0.7.0+ç§»é™¤äº†__version__å±æ€§)
        - **å¿…é¡»å®‰è£…transformers_stream_generator** (Qwen-VL-Chatå¿…éœ€ä¾èµ–)
        - å¦‚æœçœ‹åˆ°ä¾èµ–è­¦å‘Šï¼Œè¯·è¿è¡Œ: `pip install -r requirements.txt --upgrade`

        #### **é¦–æ¬¡è¿è¡Œå‡†å¤‡**
        1. **ä¸‹è½½æ¨¡å‹** (åªéœ€ä¸€æ¬¡ï¼Œéœ€è¦ç½‘ç»œ):
           ```bash
           python download_models.py
           ```
        2. **ç¡¬ä»¶è¦æ±‚**:
           - **æ¨èé…ç½®**: NVIDIA GPU (8GB+æ˜¾å­˜) + 16GB RAM
           - **æœ€ä½é…ç½®**: 8GB RAM (CPUæ¨¡å¼ï¼Œé€Ÿåº¦è¾ƒæ…¢)
           - **ç£ç›˜ç©ºé—´**: 20GB+ ç©ºé—² (Qwen-VL-Chatæ¨¡å‹çº¦18GB)

        #### **æ“ä½œæ­¥éª¤**
        1. åœ¨è¾“å…¥æ¡†å¡«å†™**å›¾ç‰‡æ–‡ä»¶å¤¹çš„ç»å¯¹è·¯å¾„**
           - Windows: `C:\\Users\\YourName\\Pictures`
           - Mac/Linux: `/home/username/Pictures`
        2. æ ¹æ®ç¡¬ä»¶æƒ…å†µé€‰æ‹©:
           - ä½æ˜¾å­˜GPU: å‹¾é€‰ **"å¯ç”¨4-bité‡åŒ–"**
           - æ— GPU/ä¸ç¨³å®š: å‹¾é€‰ **"å¼ºåˆ¶CPUæ¨¡å¼"**
        3. ç‚¹å‡» **"ğŸš€ å¼€å§‹ä¸­æ–‡æ‰“æ ‡"**
        4. å¤„ç†å®Œæˆåï¼Œæ¯ä¸ªå›¾ç‰‡åŒç›®å½•ç”Ÿæˆ **åŒå.txtæ–‡ä»¶**

        #### **ç»“æœç¤ºä¾‹**
        ```
        è¿™å¼ ç…§ç‰‡å±•ç¤ºäº†ä¸€ä¸ªé˜³å…‰æ˜åªšçš„æ˜¥æ—¥å…¬å›­åœºæ™¯ã€‚å‰æ™¯æœ‰ä¸‰ä¸ªå­©å­åœ¨è‰åœ°ä¸Šæ”¾é£ç­ï¼Œé£ç­æ˜¯é²œè‰³çš„çº¢è‰²å’Œè“è‰²ã€‚èƒŒæ™¯å¯è§ç››å¼€çš„æ¨±èŠ±æ ‘å’Œä¸€æ¡èœ¿èœ’çš„å°å¾„ã€‚è¿œå¤„æœ‰å‡ ä½è€äººååœ¨é•¿æ¤…ä¸Šä¼‘æ¯ã€‚æ•´ä½“æ°›å›´æ¸©é¦¨å’Œè°ï¼Œä½“ç°äº†æ˜¥å¤©çš„ç”Ÿæœºä¸æ´»åŠ›ã€‚
        ```

        #### **å¸¸è§é—®é¢˜**
        - **"CUDA out of memory"**: å‹¾é€‰4-bité‡åŒ–æˆ–å…³é—­å…¶ä»–ç¨‹åº
        - **æ¨¡å‹åŠ è½½å¤±è´¥**: é‡æ–°è¿è¡Œ `python download_models.py`
        - **å¤„ç†é€Ÿåº¦æ…¢**: 
          - GPUæ¨¡å¼: æ¯å¼ 3-5ç§’
          - CPUæ¨¡å¼: æ¯å¼ 20-40ç§’
        - **ä¸­æ–‡ä¹±ç **: ç”¨è®°äº‹æœ¬æˆ–VSCodeä»¥UTF-8ç¼–ç æ‰“å¼€txtæ–‡ä»¶
        - **ä¾èµ–é—®é¢˜**: ç¡®ä¿æ‰€æœ‰å¿…éœ€ä¾èµ–å·²å®‰è£…
        """)

        # é¡µè„š
        gr.Markdown(
            "<div style='text-align: center; margin-top: 20px; color: #888;'>"
            "Â© 2026 é€šä¹‰åƒé—®ç¦»çº¿å›¾ç‰‡æ‰“æ ‡å·¥å…· | å®Œå…¨ç¦»çº¿ Â· éšç§å®‰å…¨ Â· å¼€æºå…è´¹<br>"
            "ä½¿ç”¨Qwen-VL-Chatæ¨¡å‹ï¼Œéµå¾ªApache 2.0å¼€æºåè®®"
            "</div>",
            elem_classes=["footer"]
        )

    return demo


def main():
    """ä¸»å‡½æ•°"""
    global global_use_4bit

    parser = argparse.ArgumentParser(description='é€šä¹‰åƒé—®ç¦»çº¿å›¾ç‰‡æ‰“æ ‡å·¥å…·')
    parser.add_argument('--4bit', action='store_true', help='å¯ç”¨4-bité‡åŒ–æ¨¡å¼')
    parser.add_argument('--cpu', action='store_true', help='å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼')
    parser.add_argument('--port', type=int, default=9527, help='Web UIç«¯å£å·')
    args = parser.parse_args()

    global_use_4bit = args.__dict__['4bit']

    if args.__dict__['4bit']:
        print("âš¡ å¯åŠ¨4-bité‡åŒ–æ¨¡å¼ (ä½æ˜¾å­˜éœ€æ±‚)")
    if args.cpu:
        global device
        device = "cpu"
        print("ğŸ’» å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼ (æ— GPUåŠ é€Ÿ)")

    print("=" * 60)
    print("ğŸ–¼ï¸  é€šä¹‰åƒé—®ç¦»çº¿å›¾ç‰‡ä¸­æ–‡æ‰“æ ‡å·¥å…·")
    print("âœ… é€‚é…Qwen-VL-Chatå®é™…æ–‡ä»¶ç»“æ„ (æ— éœ€special_tokens_map.json)")
    print("âœ… ä¿®å¤transformers_stream_generatorä¾èµ–ç¼ºå¤±é—®é¢˜")
    print("âœ… ä¿®å¤tiktoken 0.7.0+ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜")
    print("âœ… ä¿®å¤NumPy 2.xå…¼å®¹æ€§é—®é¢˜ (å›ºå®šNumPy 1.26.4)")
    print("=" * 60)

    # æ£€æŸ¥NumPyç‰ˆæœ¬
    try:
        import numpy as np
        if not np.__version__.startswith("1.26"):
            print(f"âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°NumPy {np.__version__}")
            print("ğŸ’¡ å»ºè®®è¿è¡Œ: pip install numpy==1.26.4 --upgrade")
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥NumPyï¼Œè¯·å®‰è£…ä¾èµ–")

    # æ£€æŸ¥ç³»ç»Ÿèµ„æº
    check_system_resources()

    # åˆ›å»ºå¹¶å¯åŠ¨UI
    demo = create_ui()
    demo.launch(
        server_name="127.0.0.1",
        server_port=args.port,
        share=False,
        show_error=True,
        quiet=True
    )


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºå·²å®‰å…¨é€€å‡º")
    except Exception as e:
        print(f"âŒ ä¸¥é‡é”™è¯¯: {str(e)}")
        import traceback

        traceback.print_exc()
        sys.exit(1)