#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen-VL-Chat æ¨¡å‹ä¸‹è½½è„šæœ¬ (å•çº¿ç¨‹ç‰ˆæœ¬ - å…¼å®¹æœ€æ–°Hugging Face Hub)
- ä¸€ä¸ªæ–‡ä»¶ä¸‹è½½å®Œæˆåå†ä¸‹è½½ä¸‹ä¸€ä¸ª
- é€‚é…å®é™…æ–‡ä»¶ç»“æ„ (æ— éœ€special_tokens_map.json)
- ä¿®å¤NumPy 2.xå…¼å®¹æ€§é—®é¢˜
- å…¼å®¹æœ€æ–°Hugging Face Hub API (ç§»é™¤å·²å¼ƒç”¨å‚æ•°)
- ä¸­å›½é•œåƒæºåŠ é€Ÿ
"""

import os
import sys
import time
from pathlib import Path
import argparse
import json
import platform
import math
from tqdm import tqdm
import requests
import warnings
from urllib.parse import urlparse
from typing import List, Dict, Optional, Tuple

# å¿½ç•¥Hugging Face Hubçš„å¼ƒç”¨è­¦å‘Š
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)


def check_numpy_compatibility():
    """æ£€æŸ¥NumPyç‰ˆæœ¬å…¼å®¹æ€§"""
    try:
        import numpy as np
        numpy_version = np.__version__
        print(f"ğŸ” æ£€æµ‹åˆ°NumPyç‰ˆæœ¬: {numpy_version}")

        # æ£€æŸ¥æ˜¯å¦ä¸ºNumPy 2.x
        if numpy_version.startswith('2'):
            print(f"âš ï¸  è­¦å‘Š: NumPy {numpy_version} å¯èƒ½ä¸PyTorchä¸å…¼å®¹")
            print("ğŸ’¡ å»ºè®®åœ¨å®‰è£…ä¾èµ–æ—¶ä½¿ç”¨: pip install numpy==1.26.4 --upgrade")
            return False
        return True
    except ImportError:
        print("â„¹ï¸  NumPyæœªå®‰è£…ï¼Œå°†åœ¨ç¯å¢ƒè®¾ç½®æ—¶è‡ªåŠ¨å®‰è£…å…¼å®¹ç‰ˆæœ¬")
        return True


def get_model_file_list(repo_id: str, token: Optional[str] = None) -> List[Dict]:
    """è·å–æ¨¡å‹ä»“åº“ä¸­çš„æ–‡ä»¶åˆ—è¡¨ (å…¼å®¹æœ€æ–°API)"""
    print("ğŸ“‹ è·å–æ¨¡å‹æ–‡ä»¶åˆ—è¡¨...")

    # ä½¿ç”¨æœ€æ–°Hugging Face Hub API
    try:
        from huggingface_hub import HfApi
        api = HfApi(token=token)

        # è·å–ä»“åº“ä¿¡æ¯
        files = api.list_repo_files(repo_id=repo_id, repo_type="model")

        # è¿‡æ»¤éœ€è¦ä¸‹è½½çš„æ–‡ä»¶
        filtered_files = []
        for filename in files:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ¹é…éœ€è¦çš„æ¨¡å¼
            if any(filename.endswith(ext) for ext in [
                ".bin", ".safetensors", ".json", ".txt", ".py",
                ".tiktoken", ".md", ".png", ".model"
            ]):
                # æ’é™¤ä¸éœ€è¦çš„æ–‡ä»¶
                if any(x in filename for x in [".h5", ".ot", ".msgpack", ".onnx", ".pt"]):
                    continue

                # å°è¯•è·å–æ–‡ä»¶å¤§å°
                try:
                    file_info = api.repo_info(repo_id=repo_id, files_metadata=True)
                    # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„é€»è¾‘è·å–æ–‡ä»¶å¤§å°
                    size = 0
                except:
                    size = 0

                filtered_files.append({
                    'path': filename,
                    'size': size
                })

        print(f"âœ… æ‰¾åˆ° {len(filtered_files)} ä¸ªéœ€è¦ä¸‹è½½çš„æ–‡ä»¶")
        # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼ˆå¤§æ–‡ä»¶å…ˆä¸‹è½½ï¼‰
        filtered_files.sort(key=lambda x: x['size'], reverse=True)
        return filtered_files

    except Exception as e:
        print(f"âŒ è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
        print("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è·å–æ–‡ä»¶åˆ—è¡¨...")
        return get_model_file_list_backup(repo_id, token)


def get_model_file_list_backup(repo_id: str, token: Optional[str] = None) -> List[Dict]:
    """å¤‡ç”¨æ–¹æ³•è·å–æ–‡ä»¶åˆ—è¡¨"""
    # æ‰‹åŠ¨å®šä¹‰Qwen-VL-Chatçš„å…³é”®æ–‡ä»¶
    essential_files = [
        "config.json",
        "configuration_qwen.py",
        "generation_config.json",
        "modeling_qwen.py",
        "pytorch_model.bin.index.json",
        "qwen.tiktoken",
        "tokenizer_config.json",
        "tokenization_qwen.py",
        "README.md",
        "special_tokens_map.json"
    ]

    # åˆ†ç‰‡æ–‡ä»¶ (1-10)
    shard_files = [f"pytorch_model-{str(i).zfill(5)}-of-00010.bin" for i in range(1, 11)]

    all_files = essential_files + shard_files

    file_list = []

    print("ğŸ”§ ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è·å–æ–‡ä»¶åˆ—è¡¨...")
    for filename in tqdm(all_files, desc="æ£€æŸ¥æ–‡ä»¶ä¿¡æ¯"):
        file_list.append({
            'path': filename,
            'size': 0  # å¤§å°æœªçŸ¥
        })

    return file_list


def download_file(
        repo_id: str,
        filename: str,
        save_dir: str,
        use_mirror: bool = False,
        token: Optional[str] = None,
) -> bool:
    """å•æ–‡ä»¶ä¸‹è½½å‡½æ•°ï¼Œå…¼å®¹æœ€æ–°Hugging Face Hub API"""
    from huggingface_hub import hf_hub_download, get_hf_file_metadata

    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir_path = Path(save_dir)
    save_dir_path.mkdir(parents=True, exist_ok=True)
    save_path = save_dir_path / filename

    # è®¾ç½®é•œåƒæº
    if use_mirror:
        os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
        print("ğŸŒ ä½¿ç”¨HuggingFaceé•œåƒæº (ä¸­å›½åŠ é€Ÿ)")

    # å‡†å¤‡ä¸‹è½½
    print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½: {filename}")
    print(f"   ä¿å­˜åˆ°: {save_path}")

    try:
        # ä½¿ç”¨æœ€æ–°APIä¸‹è½½æ–‡ä»¶
        file_path = hf_hub_download(
            repo_id=repo_id,
            filename=filename,
            local_dir=save_dir_path,  # ä½¿ç”¨local_dirå‚æ•°
            token=token,
            force_download=False,  # ä¸å¼ºåˆ¶é‡æ–°ä¸‹è½½
            cache_dir=None  # ä¸ä½¿ç”¨ç¼“å­˜
        )

        # éªŒè¯æ–‡ä»¶å¤§å°
        file_size = Path(file_path).stat().st_size
        print(f"âœ… ä¸‹è½½å®Œæˆ: {filename}")
        print(f"   ä¿å­˜åˆ°: {file_path}")
        print(f"   å¤§å°: {file_size / 1024 ** 3:.2f}GB")
        return True

    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {filename}")
        print(f"   é”™è¯¯: {str(e)}")

        # æ¸…ç†ä¸å®Œæ•´çš„æ–‡ä»¶
        if save_path.exists() and save_path.stat().st_size == 0:
            save_path.unlink()

        return False


def single_threaded_download(
        repo_id: str,
        save_dir: str,
        use_mirror: bool = False,
        token: Optional[str] = None,
) -> bool:
    """å•çº¿ç¨‹ä¸‹è½½æ‰€æœ‰æ–‡ä»¶ (å…¼å®¹æœ€æ–°API)"""
    print(f"ğŸš€ å¼€å§‹å•çº¿ç¨‹ä¸‹è½½: {repo_id}")
    print(f"ğŸ“ ä¿å­˜ç›®å½•: {os.path.abspath(save_dir)}")

    # è·å–æ–‡ä»¶åˆ—è¡¨
    file_list = get_model_file_list(repo_id, token)
    if not file_list:
        print("âŒ æ— æ³•è·å–æ–‡ä»¶åˆ—è¡¨ï¼Œä¸‹è½½å¤±è´¥")
        return False

    print(f"ğŸ“‹ æ€»å…±éœ€è¦ä¸‹è½½ {len(file_list)} ä¸ªæ–‡ä»¶")

    # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼ˆå¤§æ–‡ä»¶å…ˆä¸‹è½½ï¼‰
    file_list.sort(key=lambda x: x['size'], reverse=True)

    # é€ä¸ªä¸‹è½½æ–‡ä»¶
    successful = []
    failed = []
    skipped = []

    for i, file_info in enumerate(file_list, 1):
        filename = file_info['path']
        save_path = Path(save_dir) / filename

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if save_path.exists():
            print(f"â­ è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶: {filename}")
            skipped.append(filename)
            continue

        # ä¸‹è½½æ–‡ä»¶
        print(f"\nğŸ“Š è¿›åº¦: {i}/{len(file_list)} ({i / len(file_list) * 100:.1f}%)")
        success = download_file(repo_id, filename, save_dir, use_mirror, token)

        if success:
            successful.append(filename)
        else:
            failed.append(filename)

        # æ¯ä¸‹è½½5ä¸ªæ–‡ä»¶æ¸…ç†ä¸€æ¬¡ç³»ç»Ÿç¼“å­˜ï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
        if len(successful) % 5 == 0:
            import gc
            gc.collect()

        # æ˜¾ç¤ºè¿›åº¦
        print(f"\nğŸ“Š å½“å‰è¿›åº¦: {len(successful)}/{len(file_list)} æˆåŠŸ, {len(failed)} å¤±è´¥, {len(skipped)} è·³è¿‡")

    # ç”Ÿæˆä¸‹è½½æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ‰ ä¸‹è½½ä»»åŠ¡å®Œæˆ!")
    print(f"âœ… æˆåŠŸ: {len(successful)} ä¸ªæ–‡ä»¶")
    print(f"âŒ å¤±è´¥: {len(failed)} ä¸ªæ–‡ä»¶")
    print(f"â­ è·³è¿‡: {len(skipped)} ä¸ªæ–‡ä»¶")

    if successful:
        total_size = sum((Path(save_dir) / f).stat().st_size for f in successful if (Path(save_dir) / f).exists())
        print(f"ğŸ“¦ æ€»ä¸‹è½½å¤§å°: {total_size / 1024 ** 3:.2f}GB")

    if failed:
        print("\nâš ï¸  å¤±è´¥æ–‡ä»¶åˆ—è¡¨:")
        for f in failed:
            print(f"   â€¢ {f}")
        print("\nğŸ’¡ å»ºè®®é‡æ–°è¿è¡Œä¸‹è½½è„šæœ¬ï¼Œå°†è‡ªåŠ¨ç»§ç»­ä¸‹è½½å¤±è´¥çš„æ–‡ä»¶")

    print("=" * 60)
    return len(failed) == 0


def verify_model_files(model_dir: str) -> Tuple[bool, str]:
    """æ™ºèƒ½éªŒè¯Qwen-VL-Chatæ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´"""
    model_dir_path = Path(model_dir)

    if not model_dir_path.exists():
        return False, "æ¨¡å‹ç›®å½•ä¸å­˜åœ¨"

    # å¿…éœ€çš„æ ¸å¿ƒé…ç½®æ–‡ä»¶
    required_config_files = [
        "config.json",
        "tokenizer_config.json",
        "qwen.tiktoken",
        "tokenization_qwen.py"
    ]

    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    missing_configs = [f for f in required_config_files if not (model_dir_path / f).exists()]
    if missing_configs:
        return False, f"ç¼ºå¤±å…³é”®é…ç½®æ–‡ä»¶: {missing_configs}"

    # æ£€æŸ¥æ¨¡å‹æƒé‡æ–‡ä»¶ - æ”¯æŒåˆ†ç‰‡
    model_files = list(model_dir_path.glob("pytorch_model*.bin"))

    if not model_files:
        return False, "æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶ (pytorch_model*.bin)"

    # æ£€æŸ¥åˆ†ç‰‡æ•°é‡
    shard_files = [f for f in model_files if "pytorch_model-" in f.name]
    if len(shard_files) < 8:  # å…è®¸ç¼ºå¤±1-2ä¸ª
        return False, f"ä»…æ‰¾åˆ° {len(shard_files)} ä¸ªæƒé‡åˆ†ç‰‡ï¼Œæ¨¡å‹å¯èƒ½ä¸å®Œæ•´ (åº”æœ‰10ä¸ªåˆ†ç‰‡)"

    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    total_size = sum(f.stat().st_size for f in model_files)
    if total_size < 15e9:  # 15GB
        return False, f"æ¨¡å‹æ–‡ä»¶æ€»å¤§å°è¿‡å° ({total_size / 1e9:.2f}GB)ï¼Œå¯èƒ½ä¸‹è½½ä¸å®Œæ•´ (å®Œæ•´æ¨¡å‹çº¦18GB)"

    return True, f"âœ… Qwen-VL-Chatæ¨¡å‹éªŒè¯æˆåŠŸ!\n   â€¢ æ‰¾åˆ° {len(model_files)} ä¸ªæƒé‡æ–‡ä»¶\n   â€¢ æ€»å¤§å°: {total_size / 1e9:.2f}GB"


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ä¸‹è½½Qwen-VL-Chatæ¨¡å‹ (å•çº¿ç¨‹ç‰ˆ - å…¼å®¹æœ€æ–°API)')
    parser.add_argument('--mirror', action='store_true', help='ä½¿ç”¨ä¸­å›½é•œåƒæºåŠ é€Ÿä¸‹è½½')
    parser.add_argument('--dir', type=str, default="./qwen_models", help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--token', type=str, default=None, help='HuggingFace token (å¯é€‰)')
    args = parser.parse_args()

    print("=" * 60)
    print("ğŸŒ Qwen-VL-Chat æ¨¡å‹ä¸‹è½½å·¥å…· (å•çº¿ç¨‹ç‰ˆ)")
    print("âœ… é€‚é…å®é™…æ–‡ä»¶ç»“æ„ (æ— éœ€special_tokens_map.json)")
    print("âœ… å…¼å®¹æœ€æ–°Hugging Face Hub API (ç§»é™¤å·²å¼ƒç”¨å‚æ•°)")
    print("âœ… ä¸€ä¸ªæ–‡ä»¶ä¸‹è½½å®Œæˆåå†ä¸‹è½½ä¸‹ä¸€ä¸ª")
    print("=" * 60)

    # æ£€æŸ¥NumPyå…¼å®¹æ€§
    check_numpy_compatibility()

    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    try:
        import psutil

        disk = psutil.disk_usage(os.path.abspath("."))
        free_gb = disk.free / (1024 ** 3)
        print(f"ğŸ’¾ å¯ç”¨ç£ç›˜ç©ºé—´: {free_gb:.1f}GB")
        if free_gb < 20:
            print(f"âš ï¸  è­¦å‘Š: Qwen-VL-Chatæ¨¡å‹éœ€è¦çº¦18GBç©ºé—´ï¼Œå»ºè®®è‡³å°‘20GBç©ºé—²")
            if input("ç»§ç»­ä¸‹è½½? (Y/n): ").strip().lower() != 'y':
                sys.exit(0)
    except ImportError:
        print("âš ï¸  æ— æ³•æ£€æŸ¥ç£ç›˜ç©ºé—´ (psutilæœªå®‰è£…)")
        print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install psutil")

    # æ‰§è¡Œå•çº¿ç¨‹ä¸‹è½½
    success = single_threaded_download(
        repo_id="Qwen/Qwen-VL-Chat",
        save_dir=args.dir,
        use_mirror=args.mirror,
        token=args.token,
    )

    # éªŒè¯ä¸‹è½½ç»“æœ
    if success:
        print("\nğŸ” éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§...")
        is_valid, message = verify_model_files(args.dir)
        if is_valid:
            print(f"âœ… {message}")
            print("\nğŸ‰ æ¨¡å‹ä¸‹è½½å’ŒéªŒè¯æˆåŠŸ! å¯ä»¥è¿è¡Œä¸»ç¨‹åºäº†")
            sys.exit(0)
        else:
            print(f"âŒ {message}")
            print("ğŸ’¡ å»ºè®®é‡æ–°è¿è¡Œä¸‹è½½è„šæœ¬ä¿®å¤ç¼ºå¤±æ–‡ä»¶")
            sys.exit(1)
    else:
        print("\nâŒ ä¸‹è½½æœªå®Œå…¨æˆåŠŸï¼Œä½†éƒ¨åˆ†æ–‡ä»¶å·²ä¸‹è½½")
        print("ğŸ’¡ å»ºè®®é‡æ–°è¿è¡Œä¸‹è½½è„šæœ¬ï¼Œå°†è‡ªåŠ¨ç»§ç»­ä¸‹è½½")
        sys.exit(1)