#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šä¹‰åƒé—®ç¦»çº¿å›¾ç‰‡æ‰“æ ‡å·¥å…· - å®‰è£…éªŒè¯è„šæœ¬
- éªŒè¯æ‰€æœ‰ä¾èµ–å…¼å®¹æ€§
- æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
- æµ‹è¯•æ¨¡å‹åŠ è½½åŠŸèƒ½
- è¯Šæ–­å¸¸è§é—®é¢˜
- æä¾›ä¿®å¤å»ºè®®
"""

import os
import sys
import time
import json
import platform
import argparse
import warnings
from pathlib import Path
import hashlib

# å¿½ç•¥å¼ƒç”¨è­¦å‘Š
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# å…¨å±€å˜é‡
model_path = "./qwen_models"
required_packages = [
    "torch", "transformers", "gradio", "PIL", "numpy",
    "sentencepiece", "psutil", "huggingface_hub", "tqdm",
    "tiktoken", "transformers_stream_generator", "bitsandbytes"
]


def print_header():
    """æ‰“å°è„šæœ¬æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯"""
    print("=" * 60)
    print("ğŸ” é€šä¹‰åƒé—®ç¦»çº¿å›¾ç‰‡æ‰“æ ‡å·¥å…· - å®‰è£…éªŒè¯è„šæœ¬")
    print(f"   Pythonç‰ˆæœ¬: {platform.python_version()}")
    print(f"   æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    print(f"   å½“å‰ç›®å½•: {os.getcwd()}")
    print("=" * 60)


def check_dependencies():
    """æ£€æŸ¥æ‰€æœ‰å¿…éœ€çš„ä¾èµ–æ˜¯å¦å®‰è£…"""
    print("\n" + "=" * 60)
    print("ğŸ“¦ ä¾èµ–æ£€æŸ¥")
    print("=" * 60)

    missing_packages = []
    incompatible_packages = []

    for package in required_packages:
        try:
            module = __import__(package)
            version = getattr(module, '__version__', 'æœªçŸ¥')
            print(f"âœ… {package} ç‰ˆæœ¬: {version}")

            # ç‰¹æ®Šç‰ˆæœ¬æ£€æŸ¥
            if package == "numpy":
                if not version.startswith("1.26"):
                    print(f"âš ï¸  è­¦å‘Š: NumPy {version} å¯èƒ½ä¸PyTorchä¸å…¼å®¹ (éœ€è¦1.26.4)")
                    incompatible_packages.append(package)
            elif package == "torch":
                if not version.startswith("2.2"):
                    print(f"âš ï¸  è­¦å‘Š: PyTorch {version} å¯èƒ½ä¸å…¼å®¹ (éœ€è¦2.2.0)")
                    incompatible_packages.append(package)
            elif package == "transformers":
                if not version.startswith("4.37"):
                    print(f"âš ï¸  è­¦å‘Š: Transformers {version} å¯èƒ½ä¸å…¼å®¹ (éœ€è¦4.37.0)")
                    incompatible_packages.append(package)
            elif package == "tiktoken":
                # 0.7.0+ç§»é™¤äº†__version__å±æ€§ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                try:
                    hasattr(tiktoken, '__version__')
                except:
                    print(f"âš ï¸  è­¦å‘Š: tiktoken {version} å¯èƒ½ä¸å…¼å®¹ (éœ€è¦0.6.0)")
                    incompatible_packages.append(package)

        except ImportError as e:
            print(f"âŒ {package} æœªå®‰è£…: {str(e)}")
            missing_packages.append(package)
        except Exception as e:
            print(f"âš ï¸  {package} å¯¼å…¥é”™è¯¯: {str(e)}")
            missing_packages.append(package)

    if missing_packages:
        print(f"\nâŒ å‘ç° {len(missing_packages)} ä¸ªç¼ºå¤±åŒ…: {missing_packages}")
        print("ğŸ’¡ ä¿®å¤å‘½ä»¤:")
        print("   pip install -r requirements.txt --upgrade")

    if incompatible_packages:
        print(f"\nâš ï¸  å‘ç° {len(incompatible_packages)} ä¸ªä¸å…¼å®¹åŒ…: {incompatible_packages}")
        print("ğŸ’¡ ä¿®å¤å‘½ä»¤:")
        print("   pip install numpy==1.26.4 torch==2.2.0 transformers==4.37.0 tiktoken==0.6.0 --upgrade")
        print("   pip install transformers_stream_generator==0.0.4 --upgrade")

    return len(missing_packages) == 0 and len(incompatible_packages) == 0


def check_system_resources():
    """æ£€æŸ¥ç³»ç»Ÿèµ„æºæ˜¯å¦æ»¡è¶³è¦æ±‚"""
    print("\n" + "=" * 60)
    print("ğŸ’» ç³»ç»Ÿèµ„æºæ£€æŸ¥")
    print("=" * 60)

    try:
        import psutil
        import torch

        # æ£€æŸ¥ç£ç›˜ç©ºé—´
        disk = psutil.disk_usage(os.path.abspath("."))
        free_gb = disk.free / (1024 ** 3)
        print(f"ğŸ’¾ ç£ç›˜ç©ºé—´: {free_gb:.1f}GB å¯ç”¨")
        disk_ok = free_gb >= 8

        # æ£€æŸ¥å†…å­˜
        mem = psutil.virtual_memory()
        available_gb = mem.available / (1024 ** 3)
        total_gb = mem.total / (1024 ** 3)
        print(f"ğŸ§  ç³»ç»Ÿå†…å­˜: {available_gb:.1f}GB/{total_gb:.1f}GB å¯ç”¨")
        mem_ok = available_gb >= 4

        # æ£€æŸ¥GPU
        gpu_available = torch.cuda.is_available()
        if gpu_available:
            gpu_name = torch.cuda.get_device_name(0)
            gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
            print(f"ğŸ® GPU: {gpu_name} ({gpu_mem:.1f}GB æ˜¾å­˜)")
            gpu_ok = gpu_mem >= 6
        else:
            print("âš ï¸  æœªæ£€æµ‹åˆ°NVIDIA GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            gpu_ok = True  # CPUæ¨¡å¼ä¸éœ€è¦GPU

        # æ±‡æ€»
        resources_ok = disk_ok and mem_ok and gpu_ok
        if not resources_ok:
            print("\nâš ï¸  èµ„æºè­¦å‘Š:")
            if not disk_ok:
                print(f"   â€¢ ç£ç›˜ç©ºé—´ä¸è¶³! å»ºè®®è‡³å°‘8GBç©ºé—² (å½“å‰: {free_gb:.1f}GB)")
            if not mem_ok:
                print(f"   â€¢ å¯ç”¨å†…å­˜ä¸è¶³! å»ºè®®è‡³å°‘4GB (å½“å‰: {available_gb:.1f}GB)")
            if not gpu_ok and gpu_available:
                print(f"   â€¢ GPUæ˜¾å­˜ä¸è¶³! å»ºè®®è‡³å°‘6GB (å½“å‰: {gpu_mem:.1f}GB)")

        return resources_ok

    except ImportError as e:
        print(f"âŒ æ— æ³•æ£€æŸ¥ç³»ç»Ÿèµ„æº: {str(e)}")
        print("ğŸ’¡ è¯·å®‰è£…psutil: pip install psutil")
        return False


def verify_model_files():
    """éªŒè¯Qwen-VL-Chatæ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´"""
    print("\n" + "=" * 60)
    print("ğŸ” æ¨¡å‹æ–‡ä»¶éªŒè¯")
    print("=" * 60)

    model_dir = Path(model_path)

    if not model_dir.exists():
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir.absolute()}")
        print("ğŸ’¡ è¯·å…ˆä¸‹è½½æ¨¡å‹: python download_models.py")
        return False

    print(f"ğŸ“ æ¨¡å‹ç›®å½•: {model_dir.absolute()}")

    # å¿…éœ€çš„æ ¸å¿ƒé…ç½®æ–‡ä»¶
    required_files = [
        "config.json",
        "tokenizer_config.json",
        "qwen.tiktoken",
        "tokenization_qwen.py",
        "modeling_qwen.py",
        "configuration_qwen.py"
    ]

    # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
    missing_files = []
    for file in required_files:
        if not (model_dir / file).exists():
            missing_files.append(file)

    if missing_files:
        print(f"âŒ ç¼ºå¤±å¿…éœ€æ–‡ä»¶: {missing_files}")

    # æ£€æŸ¥æƒé‡æ–‡ä»¶
    weight_files = list(model_dir.glob("pytorch_model*.bin"))
    if not weight_files:
        print("âŒ æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶ (pytorch_model*.bin)")
    else:
        print(f"âœ… æ‰¾åˆ° {len(weight_files)} ä¸ªæƒé‡æ–‡ä»¶")

        # æ£€æŸ¥åˆ†ç‰‡æ•°é‡
        shard_files = [f for f in weight_files if "pytorch_model-" in f.name]
        expected_shards = 10
        if len(shard_files) < expected_shards - 1:  # å…è®¸ç¼ºå¤±1ä¸ª
            print(f"âš ï¸  ä»…æ‰¾åˆ° {len(shard_files)}/{expected_shards} ä¸ªæƒé‡åˆ†ç‰‡ï¼Œæ¨¡å‹å¯èƒ½ä¸å®Œæ•´")

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        total_size = sum(f.stat().st_size for f in weight_files)
        print(f"ğŸ“¦ æƒé‡æ–‡ä»¶æ€»å¤§å°: {total_size / 1024 ** 3:.2f}GB")

        # éªŒè¯æœ€å°å¤§å° (çº¦15GB)
        min_size_gb = 15
        if total_size < min_size_gb * 1024 ** 3:
            print(f"âš ï¸  æ¨¡å‹æ€»å¤§å°è¿‡å° ({total_size / 1024 ** 3:.2f}GB)ï¼Œå®Œæ•´æ¨¡å‹åº”çº¦18GB")

    # éªŒè¯æ–‡ä»¶å“ˆå¸Œå€¼ (ç¤ºä¾‹ï¼Œå®é™…åº”ä½¿ç”¨å®˜æ–¹å“ˆå¸Œ)
    print("\nğŸ” éªŒè¯å…³é”®æ–‡ä»¶å“ˆå¸Œå€¼ (æŠ½æ ·æ£€æŸ¥)...")
    sample_files = [
        "config.json",
        "pytorch_model-00001-of-00010.bin",
        "pytorch_model-00010-of-00010.bin"
    ]

    for sample_file in sample_files:
        file_path = model_dir / sample_file
        if file_path.exists():
            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ (ç®€åŒ–ç‰ˆï¼Œå®é™…åº”æ¯”å¯¹å®˜æ–¹å€¼)
            with open(file_path, 'rb') as f:
                file_hash = hashlib.sha256(f.read(1024 * 1024)).hexdigest()[:8]  # åªè®¡ç®—å‰1MB
            print(f"âœ… {sample_file}: å“ˆå¸Œå‰ç¼€ {file_hash}")
        else:
            print(f"âš ï¸  {sample_file} ä¸å­˜åœ¨ï¼Œæ— æ³•éªŒè¯å“ˆå¸Œ")

    # æ¨¡å‹æ–‡ä»¶éªŒè¯ç»“æœ
    files_ok = len(missing_files) == 0 and len(weight_files) > 0 and total_size > 15 * 1024 ** 3
    if not files_ok:
        if missing_files:
            print("\nğŸ’¡ ä¿®å¤å»ºè®®:")
            print("   1. é‡æ–°ä¸‹è½½ç¼ºå¤±æ–‡ä»¶: python download_models.py")
            print("   2. æ£€æŸ¥ä¸‹è½½è¿‡ç¨‹æ˜¯å¦è¢«ä¸­æ–­")
            print("   3. ç¡®ä¿æœ‰è¶³å¤Ÿç£ç›˜ç©ºé—´ (è‡³å°‘20GB)")

    return files_ok


def test_model_loading(use_4bit=False, use_cpu=False):
    """æµ‹è¯•æ¨¡å‹åŠ è½½åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ğŸ§  æ¨¡å‹åŠ è½½æµ‹è¯•")
    print("=" * 60)

    try:
        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import gc

        # è®¾ç½®è®¾å¤‡
        device = "cpu" if use_cpu else ("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device.upper()}")

        # æ£€æŸ¥æ¨¡å‹ç›®å½•
        if not os.path.exists(model_path):
            print("âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
            return False

        # åŠ è½½tokenizer
        print("ğŸ”„ åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side='left',
            use_fast=False
        )
        print("âœ… TokenizeråŠ è½½æˆåŠŸ!")

        # å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°
        model_kwargs = {
            "trust_remote_code": True,
            "cache_dir": model_path,
            "device_map": "auto" if device == "cuda" else "cpu"
        }

        # 4-bité‡åŒ–
        if use_4bit and device == "cuda":
            print("âš¡ å¯ç”¨4-bité‡åŒ–æ¨¡å¼...")
            try:
                from auto_gptq import AutoGPTQForCausalLM
                model = AutoGPTQForCausalLM.from_quantized(
                    model_path,
                    device="cuda:0",
                    use_triton=False,
                    quantize_config=None
                )
                print("âœ… 4-bité‡åŒ–æ¨¡å‹åŠ è½½æˆåŠŸ!")
            except Exception as e:
                print(f"âš ï¸  4-bitåŠ è½½å¤±è´¥: {str(e)}")
                use_4bit = False
        else:
            use_4bit = False

        # æ ‡å‡†åŠ è½½
        if not use_4bit:
            print("ğŸ§  åŠ è½½æ ‡å‡†ç²¾åº¦æ¨¡å‹ (å¯èƒ½éœ€è¦1-2åˆ†é’Ÿ)...")
            start_time = time.time()
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                **model_kwargs
            ).eval()
            load_time = time.time() - start_time
            print(f"âœ… æ ‡å‡†ç²¾åº¦æ¨¡å‹åŠ è½½æˆåŠŸ! (è€—æ—¶: {load_time:.1f}ç§’)")

        # éªŒè¯æ¨¡å‹
        print("ğŸ” éªŒè¯æ¨¡å‹åŠŸèƒ½...")
        test_query = tokenizer.from_list_format([
            {'text': 'ä½ å¥½ï¼Œé€šä¹‰åƒé—®!'}
        ])

        with torch.no_grad():
            response, _ = model.chat(
                tokenizer,
                query=test_query,
                history=None,
                max_new_tokens=10,
                temperature=0.7,
                top_p=0.9
            )

        print(f"âœ… æ¨¡å‹åŠŸèƒ½éªŒè¯æˆåŠŸ! å“åº”: '{response.strip()}'")

        # æ¸…ç†èµ„æº
        del model
        del tokenizer
        if device == "cuda":
            torch.cuda.empty_cache()
        gc.collect()

        return True

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

        print("\nğŸ› ï¸  è¯¦ç»†æ•…éšœæ’é™¤:")
        print("1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§ (é‡æ–°è¿è¡Œæœ¬è„šæœ¬)")
        print("2. æ£€æŸ¥ä¾èµ–ç‰ˆæœ¬ (numpy==1.26.4, torch==2.2.0, transformers==4.37.0)")
        print("3. ç¡®ä¿å·²å®‰è£… transformers_stream_generator==0.0.4")
        print("4. å°è¯•4-bité‡åŒ–æ¨¡å¼: --4bit å‚æ•°")
        print("5. å°è¯•CPUæ¨¡å¼: --cpu å‚æ•°")
        return False


def generate_report(results):
    """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
    print("\n" + "=" * 60)
    print("ğŸ“Š éªŒè¯æŠ¥å‘Š")
    print("=" * 60)

    # è®¡ç®—æ€»ä½“é€šè¿‡ç‡
    total_checks = len(results)
    passed_checks = sum(1 for r in results.values() if r)
    pass_rate = passed_checks / total_checks * 100

    print(f"âœ… é€šè¿‡: {passed_checks}/{total_checks} é¡¹ ({pass_rate:.1f}%)")

    # æŒ‰ä¼˜å…ˆçº§æ’åºç»“æœ
    priority_order = [
        'dependencies', 'model_files', 'model_loading',
        'system_resources'
    ]

    for check in priority_order:
        if check in results:
            status = "âœ… é€šè¿‡" if results[check] else "âŒ å¤±è´¥"
            print(f"   â€¢ {check.replace('_', ' ').title()}: {status}")

    # æ€»ä½“ç»“è®º
    if passed_checks == total_checks:
        print("\nğŸ‰ éªŒè¯æˆåŠŸ! æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥è¿è¡Œä¸»ç¨‹åºäº†")
        print("ğŸš€ å¯åŠ¨å‘½ä»¤: python app.py")
    elif passed_checks >= total_checks - 1:
        print("\nâš ï¸  åŸºæœ¬å¯ç”¨! å¤§éƒ¨åˆ†æ£€æŸ¥é€šè¿‡ï¼Œä½†æœ‰è½»å¾®é—®é¢˜")
        print("ğŸ”§ å»ºè®®ä¿®å¤åå†ä½¿ç”¨: python fix_dependencies.py")
    else:
        print("\nâŒ éªŒè¯å¤±è´¥! å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œéœ€è¦ä¿®å¤")

        # æä¾›é’ˆå¯¹æ€§ä¿®å¤å»ºè®®
        if not results.get('dependencies', True):
            print("\nğŸ’¡ ä¾èµ–ä¿®å¤å»ºè®®:")
            print("   pip install -r requirements.txt --upgrade")

        if not results.get('model_files', True):
            print("\nğŸ’¡ æ¨¡å‹æ–‡ä»¶ä¿®å¤å»ºè®®:")
            print("   python download_models.py")

        if not results.get('model_loading', True):
            print("\nğŸ’¡ æ¨¡å‹åŠ è½½ä¿®å¤å»ºè®®:")
            print("   â€¢ æ£€æŸ¥NumPyç‰ˆæœ¬: pip install numpy==1.26.4 --upgrade")
            print("   â€¢ æ£€æŸ¥tiktokenç‰ˆæœ¬: pip install tiktoken==0.6.0 --upgrade")
            print("   â€¢ å®‰è£…transformers_stream_generator: pip install transformers_stream_generator==0.0.4 --upgrade")
            print("   â€¢ å°è¯•4-bité‡åŒ–: python app.py --4bit")
            print("   â€¢ å°è¯•CPUæ¨¡å¼: python app.py --cpu")

    print("\n" + "=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='éªŒè¯é€šä¹‰åƒé—®ç¦»çº¿å›¾ç‰‡æ‰“æ ‡å·¥å…·å®‰è£…')
    parser.add_argument('--4bit', action='store_true', help='æµ‹è¯•4-bité‡åŒ–æ¨¡å¼')
    parser.add_argument('--cpu', action='store_true', help='å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼æµ‹è¯•')
    parser.add_argument('--quick', action='store_true', help='å¿«é€ŸéªŒè¯ (è·³è¿‡è€—æ—¶æµ‹è¯•)')
    args = parser.parse_args()

    print_header()

    # å­˜å‚¨éªŒè¯ç»“æœ
    results = {}

    # 1. ä¾èµ–æ£€æŸ¥
    results['dependencies'] = check_dependencies()

    # 2. ç³»ç»Ÿèµ„æºæ£€æŸ¥
    results['system_resources'] = check_system_resources()

    # 3. æ¨¡å‹æ–‡ä»¶éªŒè¯
    results['model_files'] = verify_model_files()

    # 4. æ¨¡å‹åŠ è½½æµ‹è¯• (å¦‚æœæ–‡ä»¶éªŒè¯é€šè¿‡)
    if results['model_files']:
        results['model_loading'] = test_model_loading(
            use_4bit=args.__dict__['4bit'],
            use_cpu=args.__dict__['cpu']
        )
    else:
        results['model_loading'] = False
        print("\nâš ï¸  è·³è¿‡æ¨¡å‹åŠ è½½æµ‹è¯• (æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´)")

    # ç”ŸæˆæŠ¥å‘Š
    generate_report(results)

    # é€€å‡ºä»£ç  (0=æˆåŠŸ, 1=å¤±è´¥)
    overall_success = all(results[key] for key in results if results[key] is not None)
    sys.exit(0 if overall_success else 1)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ éªŒè¯å·²ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ä¸¥é‡é”™è¯¯: {str(e)}")
        import traceback

        traceback.print_exc()
        sys.exit(1)